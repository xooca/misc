{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python [conda env:NLP]",
      "language": "python",
      "name": "conda-env-NLP-py"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGQwJHEbJ7-9",
        "colab_type": "code",
        "outputId": "36bfa680-f817-485b-b8bc-f2b2ea0b8470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (3.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2.21.0)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2.8)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.3.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.5.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (2.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbuxELHxTZ3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = {0:7276,1:289829}\n",
        "b= {0:7276}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpBMqlTaUYo6",
        "colab_type": "code",
        "outputId": "7ad250e9-e7fc-459c-8e9f-bebdb2871be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(b.items())[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn669WrmIaPT",
        "colab_type": "code",
        "outputId": "0dfd54ad-059a-45f5-c80c-fc9378ca2ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.externals import joblib\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import itertools\n",
        "from sklearn.metrics import r2_score\n",
        "import sys\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import Imputer\n",
        "from catboost import CatBoostClassifier,CatBoostRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import itertools\n",
        "import sys\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "\n",
        "import gc \n",
        "os.getcwd()\n",
        "pd.options.display.max_columns = None\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjCt21s6LMf6",
        "colab_type": "code",
        "outputId": "5fa2b5f5-30fb-4fee-f88f-8cf7908947bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H-gcSrxJMY2",
        "colab_type": "code",
        "outputId": "a1c376d1-307a-40b8-c77f-d6683316bd1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/hackathons/Data Scientist Test/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Data Scientist Test.pdf'   dataset_00_with_header.csv\t lgb_1.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c1b73Rpwaog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = '/content/drive/My Drive/hackathons/Data Scientist Test/dataset_00_with_header.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFvVu5FdbgIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nullfinder(df):\n",
        "    for col in df:\n",
        "        if sum(df[col].isnull()) > 0:\n",
        "            print(f\"For {col} number of nulls are {sum(df[col].isnull())}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCG4F1B5TsEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class datacleaner:\n",
        "    def __init__(self, trainfile,targetcol,cat_threshold=100):\n",
        "        #self.df_test = pd.read_csv(testfile,index_col=0)\n",
        "        #print(self.df_test.columns)\n",
        "        self.df_train = pd.read_csv(trainfile)  \n",
        "        self.target = targetcol\n",
        "        self.dfcolumns = self.df_train.columns.tolist()\n",
        "        self.dfcolumns_nottarget = [col for col in self.dfcolumns if col != self.target]\n",
        "        self.rejectcols = []\n",
        "        self.retainedcols = []\n",
        "        self.catcols = {}\n",
        "        self.noncatcols = {}\n",
        "        self.catcols_list = []\n",
        "        self.noncatcols_list = []\n",
        "        self.hightarge_corr_col = []\n",
        "        self.threshold = cat_threshold\n",
        "        \n",
        "    def retail_reject_cols(self,threshold):\n",
        "      def retail_reject_cols_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  #print('start')\n",
        "                  for col in tmpdf:\n",
        "                    if sum(tmpdf[col].isnull()) > 0:\n",
        "                        percent_val = sum(tmpdf[col].isnull())/tmpdf[col].shape[0]\n",
        "                        #print(f\"For {col} number of nulls are {sum(df[col].isnull())} : {sum(df[col].isnull())/df[col].shape[0]}\")\n",
        "                        if percent_val > threshold:\n",
        "                          self.rejectcols.append(col)\n",
        "                  self.retainedcols = [col for col in tmpdf.columns.tolist() if col not in self.rejectcols]\n",
        "                  print(f\"Number of rejected columns {len(self.rejectcols)}\")\n",
        "                  print(f\"Number of retained columns {len(self.retainedcols)}\")\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return retail_reject_cols_lvl\n",
        "    \n",
        "    #def init_values(self,)\n",
        "    def refresh_cat_noncat_cols_fn(self,tmpdf,threshold=100):\n",
        "      try:\n",
        "          self.catcols = {}\n",
        "          self.catcols_list = []\n",
        "          self.noncatcols = {}\n",
        "          self.noncatcols_list = []\n",
        "          self.dfcolumns = tmpdf.columns.tolist()\n",
        "          self.dfcolumns_nottarget = [col for col in self.dfcolumns if col != self.target]\n",
        "          for col in self.dfcolumns_nottarget:\n",
        "            col_unique_cnt = tmpdf[col].nunique()\n",
        "            if (col_unique_cnt < threshold) and ((tmpdf[col].dtype != 'float32') and (tmpdf[col].dtype != 'float64')):\n",
        "              self.catcols.update({col:col_unique_cnt})\n",
        "              self.catcols_list.append(col)\n",
        "            else:\n",
        "              self.noncatcols.update({col:col_unique_cnt})\n",
        "              self.noncatcols_list.append(col)\n",
        "      except:\n",
        "          sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "        \n",
        "    def refresh_cat_noncat_cols(self,threshold):\n",
        "      def refresh_cat_noncat_cols_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.catcols = {}\n",
        "                  self.catcols_list = []\n",
        "                  self.noncatcols = {}\n",
        "                  self.noncatcols_list = []\n",
        "                  self.dfcolumns = tmpdf.columns.tolist()\n",
        "                  self.dfcolumns_nottarget = [col for col in self.dfcolumns if col != self.target]\n",
        "                  for col in self.dfcolumns_nottarget:\n",
        "                    col_unique_cnt = tmpdf[col].nunique()\n",
        "                    if (col_unique_cnt < threshold) and ((tmpdf[col].dtype != 'float32') and (tmpdf[col].dtype != 'float64')):\n",
        "                      self.catcols.update({col:col_unique_cnt})\n",
        "                      self.catcols_list.append(col)\n",
        "                    else:\n",
        "                      self.noncatcols.update({col:col_unique_cnt})\n",
        "                      self.noncatcols_list.append(col)\n",
        "                  print('ref'+str(len(self.noncatcols_list)))\n",
        "                  print('ref'+str(self.noncatcols_list))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return refresh_cat_noncat_cols_lvl1 \n",
        "    \n",
        "    def findcatcols(self,th):\n",
        "      for col in self.dfcolumns_nottarget:\n",
        "        col_unique_cnt = self.df_train[col].nunique()\n",
        "        if col_unique_cnt < th:\n",
        "          #print(f\"{col} is category with unique values {df[col].nunique()}\")\n",
        "          self.catcols.update({col:col_unique_cnt})\n",
        "        else:\n",
        "          self.noncatcols.update({col:col_unique_cnt})\n",
        "      print(f\"Number of categorical column is {len(self.catcols.keys())}\")\n",
        "      print(f\"Number of Non categorical column is {len(self.noncatcols.keys())}\")\n",
        "      return self.catcols,self.noncatcols\n",
        "\n",
        "  \n",
        "    def standardize_stratified(self, includestandcols=[]):\n",
        "      def standardize_stratified_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in tmpdf:\n",
        "                    if tmpdf[col].dtype == 'float32' or tmpdf[col].dtype == 'float64' or (col in includestandcols):\n",
        "                        tmpdf[col] = dtmpdff[col].replace(np.inf, 0.0)\n",
        "                        if tmpdf[col].mean() > 1000:\n",
        "                            scaler = MinMaxScaler(feature_range=(0,10))\n",
        "                            tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                        elif tmpdf[col].mean() > 100:\n",
        "                            scaler = MinMaxScaler(feature_range=(0,5))\n",
        "                            #print(col)\n",
        "                            tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                        elif tmpdf[col].mean() > 10:\n",
        "                            scaler = MinMaxScaler(feature_range=(0,2))\n",
        "                            #print(col)\n",
        "                            tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                        else:\n",
        "                            scaler = MinMaxScaler(feature_range=(0,1))\n",
        "                            #print(col)\n",
        "                            tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                        print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + col + 'is standardized')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return standardize_stratified_lvl\n",
        "    \n",
        "    def featurization(self,cat_coltype=False):\n",
        "      def featurization_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  if cat_coltype:\n",
        "                    column_list = self.catcols_list\n",
        "                  else:\n",
        "                    column_list = self.noncatcols_list\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before featurization ' + str(tmpdf.shape))\n",
        "                  for col in column_list:  \n",
        "                    tmpdf[col+'_minus_mean'] = tmpdf[col] - np.mean(tmpdf[col])\n",
        "                    tmpdf[col+'_minus_mean']  = tmpdf[col+'_minus_mean'].astype(np.float32)\n",
        "                    tmpdf[col+'_minus_max'] = tmpdf[col] - np.max(tmpdf[col])\n",
        "                    tmpdf[col+'_minus_max'] =  tmpdf[col+'_minus_max'].astype(np.float32)\n",
        "                    tmpdf[col+'_minus_min'] = tmpdf[col] - np.min(tmpdf[col])\n",
        "                    tmpdf[col+'_minus_min'] =  tmpdf[col+'_minus_min'].astype(np.float32)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after featurization ' + str(tmpdf.shape))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return featurization_lvl1\n",
        "    \n",
        "    def two_column_featurization(self,cat_coltype=False):\n",
        "      def featurization_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  if cat_coltype:\n",
        "                    column_list = self.catcols_list\n",
        "                  else:\n",
        "                    column_list = self.noncatcols_list\n",
        "                  #print('2col'+str(tmpdf.shape))\n",
        "                  #print('2col'+str(tmpdf.columns))\n",
        "                  #print('2col'+str(self.dfcolumns))\n",
        "                  #print('2col'+str(column_list))\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before two column featurization ' + str(tmpdf.shape))\n",
        "                  for col in column_list:  \n",
        "                    for col1 in column_list:\n",
        "                      if col!=col1:\n",
        "                        tmpdf[col+'_diff_'+col1]=tmpdf[col]-tmpdf[col1]\n",
        "                        tmpdf[col+'_diff_'+col1] = tmpdf[col+'_diff_'+col1].astype(np.float32)\n",
        "                        tmpdf[col+'_sum_'+col1]=tmpdf[col]+tmpdf[col1]\n",
        "                        tmpdf[col+'_sum_'+col1] = tmpdf[col+'_sum_'+col1].astype(np.float32)\n",
        "                        tmpdf[col+'_ratio_'+col1]=tmpdf[col]/tmpdf[col1]\n",
        "                        tmpdf[col+'_ratio_'+col1] =  tmpdf[col+'_ratio_'+col1].astype(np.float32)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after two column featurization ' + str(tmpdf.shape))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return featurization_lvl1\n",
        "    \n",
        "    def standardize_simple(self,cat_coltype=False,range_tuple = (0,1)):\n",
        "      def standardize_simple_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  if cat_coltype:\n",
        "                    column_list = self.catcols_list\n",
        "                  else:\n",
        "                    column_list = self.noncatcols_list\n",
        "                  scaler = MinMaxScaler(feature_range=range_tuple)\n",
        "                  #tmpdf_col = [col for col in tmpdf.columns.tolist(),]\n",
        "                  for col in column_list:  \n",
        "                    tmpdf[col] = tmpdf[col].replace(np.inf, 0.0)\n",
        "                    tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                    print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + col + 'is standardized')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return standardize_simple_lvl\n",
        "    \n",
        "    def standardize_simple_auto(self,range_tuple = (0,1)):\n",
        "      def standardize_simple_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  scaler = MinMaxScaler(feature_range=range_tuple)\n",
        "                  #tmpdf_col = [col for col in tmpdf.columns.tolist(),]\n",
        "                  for col in tmpdf:\n",
        "                    if (tmpdf[col].dtype == 'float32') or (tmpdf[col].dtype == 'float64'):   \n",
        "                      tmpdf[col] = tmpdf[col].replace(np.inf, 0.0)\n",
        "                      tmpdf[col] = tmpdf[col].replace(np.nan, 0.0)\n",
        "                      tmpdf[col] = tmpdf[col].replace(-np.inf, 0.0)\n",
        "                      tmpdf[col] = scaler.fit_transform(np.asarray(tmpdf[col]).reshape(-1,1))\n",
        "                      print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + col + 'is standardized')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return standardize_simple_lvl\n",
        "    \n",
        "    def columnmapondf(self, col_list_of_dict = [['colname',{}],['colname',{}]]):\n",
        "      def columnmapondf_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in col_list_of_dict:\n",
        "                      #print(mapdict)\n",
        "                      #print(col)\n",
        "                      convertedcolumns = tmpdf[col[0]].map(col[1])\n",
        "                      tmpdf[col] = convertedcolumns\n",
        "                      print(\"INFO : \" + str(\n",
        "                          datetime.now()) + ' : ' + 'Column ' + col[0] + ',mapped using dictionary : ' + str(col[1]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return columnmapondf_lvl\n",
        "\n",
        "    def dropcolumnfromdf(self, column_list=[]):\n",
        "      def dropcolumnfromdf_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  tmpdf = tmpdf.drop(column_list, axis=1)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + str(\n",
        "                      column_list) + ',dropped from base dataframe')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return dropcolumnfromdf_lvl\n",
        "    \n",
        "    def feature_importance(self,dfforimp,tobepredicted,modelname,featurelimit=0):\n",
        "      colname = [col for col in dfforimp.columns.tolist() if col!=tobepredicted]\n",
        "      X = dfforimp[colname]\n",
        "      y = dfforimp[tobepredicted]\n",
        "      if modelname =='rfclassifier':\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state =10)\n",
        "      elif modelname == 'rfregressor':\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state =10)\n",
        "      elif modelname =='lgbmclassifier':\n",
        "        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.05, verbose=-1)\n",
        "      elif modelname == 'lgbmregressor':\n",
        "        model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, verbose=-1)\n",
        "      model.fit(X, y)\n",
        "      feature_names = X.columns\n",
        "      feature_importances = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
        "      feature_importances = feature_importances.sort_values(by=['importance'], ascending=False).reset_index()\n",
        "      feature_importances = feature_importances[['feature', 'importance']]\n",
        "      if featurelimit == 0:\n",
        "          return feature_importances\n",
        "      else:\n",
        "          return feature_importances[:featurelimit]\n",
        "\n",
        "    def importantfeatures(self,dfforimp,tobepredicted,modelname='regressor',skipcols=[],featurelimit=0):\n",
        "      f_imp = self.feature_importance(dfforimp, tobepredicted, modelname,featurelimit)\n",
        "      allimpcols = list(f_imp['feature'])\n",
        "      stuff = []\n",
        "      for col in allimpcols:\n",
        "          for skipcol in skipcols:\n",
        "              if col != skipcol:\n",
        "                  stuff.append(col)\n",
        "              else:\n",
        "                  pass\n",
        "      return stuff,f_imp\n",
        "\n",
        "    def convertdatatypes(self,cat_threshold = 100):\n",
        "      def convertdatatypes_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  original_memory = tmpdf.memory_usage().sum()\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Size of dataset before applying converdatatypes '+str(original_memory))\n",
        "                  for c in tmpdf:\n",
        "                      if ('y' in c):\n",
        "                          tmpdf[c] = tmpdf[c].fillna(0).astype(np.int32)\n",
        "                      elif (tmpdf[c].dtype == 'object') and (tmpdf[c].nunique() < catthrshocat_thresholdld):\n",
        "                          tmpdf[c] = tmpdf[c].astype('category')   \n",
        "                      #elif (tmpdf[c].nunique() < cat_threshold):\n",
        "                      #    tmpdf[c] = tmpdf[c].astype('category') \n",
        "                      elif tmpdf[c].dtype == float:\n",
        "                          tmpdf[c] = tmpdf[c].astype(np.float32)\n",
        "                      elif tmpdf[c].dtype == int:\n",
        "                          tmpdf[c] = tmpdf[c].astype(np.int32)\n",
        "                  new_memory = tmpdf.memory_usage().sum()\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Size of dataset after applying converdatatypes '+str(new_memory))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "\n",
        "      return convertdatatypes_lvl\n",
        "\n",
        "    def sumtwocolumns(self, combination_list=[['col1','col2','col1+col2']]):\n",
        "      def sumtwocolumns_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in combination_list:\n",
        "                      tmpdf[col[2]] = tmpdf[col[0]] + tmpdf[col[1]]\n",
        "                      print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Columns ' + col[0] + ' and ' + col[\n",
        "                          1] + ' added and the name of new column is ' + str(totcol[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return sumtwocolumns_lvl\n",
        "\n",
        "    def difftwocolumns(self, combination_list=[['col1','col2','col1+col2']]):\n",
        "\n",
        "      def difftwocolumns_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in combination_list:\n",
        "                      tmpdf[col[2]] = tmpdf[col[0]] + tmpdf[col[1]]\n",
        "                      print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Columns ' + col[0] + ' and ' + col[\n",
        "                          1] + ' subtracted and the name of new column is ' + str(totcol[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return difftwocolumns_lvl\n",
        "\n",
        "    def ohe_on_threshold(self, threshold,drop_converted_col=True):\n",
        "      def ohe_on_threshold_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in tmpdf.columns.tolist():\n",
        "                    if tmpdf[col].nunique() < threshold:\n",
        "                      dummy = pd.get_dummies(tmpdf[col])\n",
        "                      tmpdf = pd.concat([tmpdf, dummy], axis=1)\n",
        "                      if drop_converted_col:\n",
        "                        tmpdf = tmpdf.drop(col,axis=1)\n",
        "                      print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + col + ' converted to dummies')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return ohe_on_threshold_lvl1\n",
        "    \n",
        "\n",
        "    def remove_collinear(self, th=0.95):\n",
        "      def converttodummies_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before collinear drop ' + str(tmpdf.shape))\n",
        "                  corr_matrix = tmpdf.corr().abs()\n",
        "                  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "                  to_drop = [column for column in upper.columns if any(upper[column] > th)]\n",
        "                  tmpdf  = tmpdf.drop(to_drop, axis=1)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after collinear drop ' + str(tmpdf.shape))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return converttodummies_lvl\n",
        "    \n",
        "    def high_coor_target_column(self,targetcol = 'y',th=0.5):\n",
        "      def high_coor_target_column_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before retaining only highly corelated col with target ' + str(tmpdf.shape))\n",
        "                  cols = [col for col in tmpdf.columns.tolist() if col != targetcol]\n",
        "                  for col in cols:\n",
        "                    tmpdcorr = tmpdf[col].corr(tmpdf[targetcol])\n",
        "                    if tmpdcorr > th:\n",
        "                      self.hightarge_corr_col.append(col)\n",
        "                  cols = self.hightarge_corr_col + [targetcol]\n",
        "                  tmpdf = tmpdf[cols]\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after retaining only highly corelated col with target ' + str(tmpdf.shape))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return high_coor_target_column_lvl1      \n",
        "    \n",
        "    def ohe_on_column(self,drop_converted_col=True):\n",
        "      def converttodummies_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  #self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  column_list = self.catcols_list\n",
        "                  for col in column_list:\n",
        "                    dummy = pd.get_dummies(tmpdf[col])\n",
        "                    dummy.columns = [col.lower()+'_'+str(x).lower().strip() for x in dummy.columns]\n",
        "                    tmpdf = pd.concat([tmpdf, dummy], axis=1)\n",
        "                    #tmpdf.columns = [col.lower()+'_'+str(x).lower().strip() for x in tmpdf.columns]\n",
        "                    if drop_converted_col:\n",
        "                      tmpdf = tmpdf.drop(col,axis=1)\n",
        "                    print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Column ' + col + ' converted to dummies')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return converttodummies_lvl\n",
        "    \n",
        "    def logtransform(self, logtransform_col=[]):\n",
        "      def logtransform_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in logtransform_col:\n",
        "                      tmpdf[col] = tmpdf[col].apply(lambda x: np.log(x) if x != 0 else 0)\n",
        "                      print(\"INFO : \" + str(\n",
        "                          datetime.now()) + ' : ' + 'Column ' + col + ' converted to corresponding log using formula: log(x)')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return logtransform_lvl\n",
        "\n",
        "    def applyformulaoncolumns(self, column_formula = []):\n",
        "      def applyformulaoncolumns_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col, formula in column_formula.items():\n",
        "                      formulabuilder = 'tmpdf[' + col + '].apply(lambda x: ' + formula + ' if x!=0 else 0)'\n",
        "                      tmpdf[col] = eval(formulabuilder)\n",
        "                      print(\"INFO : \" + str(\n",
        "                          datetime.now()) + ' : ' + 'Column ' + col + ' converted to corresponding log using formula:' + formula)\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper     \n",
        "      return applyformulaoncolumns_lvl\n",
        "        \n",
        "        \n",
        "\n",
        "    def applypca(self, cat_coltype = False,number_of_components = 50):\n",
        "      def applypca_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  if cat_coltype:\n",
        "                    column_list = self.catcols_list\n",
        "                  else:\n",
        "                    column_list = self.noncatcols_list\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before pca ' + str(tmpdf.shape))\n",
        "                  non_included_cols = [col for col in self.dfcolumns_nottarget if col not in column_list]\n",
        "                  non_included_cols = non_included_cols + [self.target]\n",
        "                  x = tmpdf.loc[:, column_list].values\n",
        "                  #y = tmpdf.loc[:,[self.target]].values\n",
        "                  #x = StandardScaler().fit_transform(x)\n",
        "                  pca = PCA(n_components=number_of_components)\n",
        "                  #pca = PCA()\n",
        "                  pca_comp = pca.fit_transform(x)\n",
        "                  pca_df = pd.DataFrame(data = pca_comp, columns = ['pca_'+str(i) for i in range(number_of_components)])\n",
        "                  #pca_df = pd.DataFrame(data = pca_comp)\n",
        "                  #print(non_included_cols)\n",
        "                  tmpdf = pd.concat([pca_df, tmpdf[non_included_cols]], axis = 1)\n",
        "                  #print(tmpdf1.shape)\n",
        "                  #print(non_included_cols)\n",
        "                  #print(tmpdf1[non_included_cols].shape)\n",
        "                  #tmpdf = pd.concat([tmpdf1,tmpdf[[self.target]]], axis = 1)  \n",
        "                  self.explained_variance = pca.explained_variance_ratio_\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after pca ' + str(tmpdf.shape))\n",
        "                  gc.enable()\n",
        "                  del x,pca_comp,pca_df\n",
        "                  gc.collect()\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return applypca_lvl1\n",
        "\n",
        "        \n",
        "    def applytsne(self, cat_coltype = False,number_of_components = 50):\n",
        "      def applytsne_lvl1(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  if cat_coltype:\n",
        "                    column_list = self.catcols_list\n",
        "                  else:\n",
        "                    column_list = self.noncatcols_list\n",
        "                  print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe before tsne ' + str(tmpdf.shape))\n",
        "                  tmpdf_col = tmpdf.columns.tolist()\n",
        "                  non_included_cols = [col for col in self.dfcolumns if col not in column_list]\n",
        "                  x = tmpdf.loc[:, column_list].values\n",
        "                  #y = tmpdf.loc[:,[self.target]].values\n",
        "                  #x = StandardScaler().fit_transform(x)\n",
        "                  tsne = TSNE(n_components=number_of_components)\n",
        "                  tsne_comp = tsne.fit_transform(x)\n",
        "                  tsne_df = pd.DataFrame(data = tsne_comp, columns = ['tsne_'+str(i) for i in range(number_of_components)])\n",
        "                  tmpdf = pd.concat([tsne_df, tmpdf[non_included_cols]], axis = 1)\n",
        "                  #print(tmpdf1.shape)\n",
        "                  #tmpdf = pd.concat([tmpdf1,tmpdf[[self.target]]], axis = 1)\n",
        "                  #print(\"INFO : \" + str(datetime.now()) + ' : ' + 'Shape of dataframe after tsne ' + str(tmpdf.shape))\n",
        "                  gc.enable()\n",
        "                  del x,tsne_comp,tsne_df\n",
        "                  gc.collect()\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "          return wrapper\n",
        "      return applytsne_lvl1\n",
        "        \n",
        "    def convertnulls_to(self, columns_list=[],to_val=''):\n",
        "      def convertnullsto_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  tmpdf.update(tmpdf[columns_list].fillna(to_val))\n",
        "                  print(\"INFO : \" + str(\n",
        "                          datetime.now()) + ' : ' + 'Columns : ' + columns_list + ' converted to '+str(to_val))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return convertnullsto_lvl\n",
        "\n",
        "\n",
        "    def addbackcolumn(self, stage):\n",
        "      dictargname_col = 'addbackcolumn_col_' + str(stage)\n",
        "      dictargname_map = 'addbackcolumn_map_' + str(stage)\n",
        "      addbackcolumn_col = self.colnamelistdict[dictargname_col]\n",
        "      addbackcolumn_map = self.maplistdict[dictargname_map]\n",
        "\n",
        "      def addbackcolumn_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  for col in addbackcolumn_col:\n",
        "                      if addbackcolumn_map == 'train':\n",
        "                          tmpdf = pd.concat([tmpdf, self.df_train[col]], axis=1)\n",
        "                      elif addbackcolumn_map == 'test':\n",
        "                          tmpdf = pd.concat([tmpdf, self.df_test[col]], axis=1)\n",
        "                      else:\n",
        "                          print(\"INFO : \" + str(\n",
        "                              datetime.now()) + ' : ' + 'Unable to add column.Please enter test or train in input sheet')\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return addbackcolumn_lvl\n",
        "\n",
        "    def testtrainna(self, tmpdf, tobepred_col):\n",
        "      key = tmpdf[tobepred_col].isnull()\n",
        "      df_na = tmpdf.loc[key]\n",
        "      df_notNA_train_x = pd.DataFrame()\n",
        "      df_notNA_train_y = pd.DataFrame()\n",
        "      df_NA_test_x = pd.DataFrame()\n",
        "      df_NA_test_y = pd.DataFrame()\n",
        "      try:\n",
        "          #print(df_na.shape)\n",
        "          if df_na.empty:\n",
        "              print(\n",
        "                  \"INFO : \" + str(\n",
        "                      datetime.now()) + ' : ' + 'Column ' + tobepred_col + 'has no null values so imputing cancelled')\n",
        "              return df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y\n",
        "          else:\n",
        "              df_notNA = tmpdf.loc[~key]\n",
        "              df_notNA_train_x = df_notNA.dropna(how='any')\n",
        "              df_notNA_train_y = df_notNA_train_x[tobepred_col]\n",
        "              df_notNA_train_x = df_notNA_train_x.drop(tobepred_col, axis=1)\n",
        "              df_NA_test_y = df_na[tobepred_col]\n",
        "              df_NA_test_x = df_na.drop(tobepred_col, axis=1)\n",
        "              df_NA_test_x = df_NA_test_x.ffill().bfill()\n",
        "              df_NA_test_x = df_NA_test_x.dropna(axis=1, how='all')\n",
        "              df_notNA_train_x = df_notNA_train_x[df_NA_test_x.columns]\n",
        "          return df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y\n",
        "      except:\n",
        "          sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "    def model_null_impute_notcat_dt(self):\n",
        "      def model_null_impute_notcat_dt_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  column_list = self.catcols_list\n",
        "                  for col in column_list:\n",
        "                      #print(col)\n",
        "                      df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y = self.testtrainna(tmpdf, col)\n",
        "                      if df_notNA_train_x.empty:\n",
        "                          pass\n",
        "                          #print(\"INFO : \" + str(datetime.now()) + ' : ' + 'imputing cancelled')\n",
        "                      else:\n",
        "                          dtree = DecisionTreeRegressor()\n",
        "                          #print(df_notNA_train_x.columns[df_notNA_train_x.isna().any()].tolist())\n",
        "                          #print(df_notNA_train_y.columns[df_notNA_train_y.isna().any()].tolist())\n",
        "                          #print(df_notNA_train_x.shape, df_notNA_train_y.shape)\n",
        "                          dtree.fit(df_notNA_train_x, df_notNA_train_y)\n",
        "                          predictions = dtree.predict(df_NA_test_x)\n",
        "                          df_NA_test_x[col] = predictions\n",
        "                          tmpdf[col].update(df_NA_test_x[col])\n",
        "                          print(\"INFO : \" + str(\n",
        "                              datetime.now()) + ' : ' + 'Column ' + col + ',data imputing completed : No. of data imputed = ' + str(\n",
        "                              df_NA_test_x.shape[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return model_null_impute_notcat_dt_lvl\n",
        "\n",
        "    def model_null_impute_cat_dt(self):\n",
        "      def model_null_impute_notcat_dt_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  column_list = self.noncatcols_list\n",
        "                  for col in column_list:\n",
        "                      #print(col)\n",
        "                      df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y = self.testtrainna(tmpdf, col)\n",
        "                      if df_notNA_train_x.empty:\n",
        "                          pass\n",
        "                          #print(\"INFO : \" + str(datetime.now()) + ' : ' + 'imputing cancelled')\n",
        "                      else:\n",
        "                          dtree = DecisionTreeClassifier()\n",
        "                          #print(df_notNA_train_x.columns[df_notNA_train_x.isna().any()].tolist())\n",
        "                          #print(df_notNA_train_y.columns[df_notNA_train_y.isna().any()].tolist())\n",
        "                          #print(df_notNA_train_x.shape, df_notNA_train_y.shape)\n",
        "                          dtree.fit(df_notNA_train_x, df_notNA_train_y)\n",
        "                          predictions = dtree.predict(df_NA_test_x)\n",
        "                          #print(predictions)\n",
        "                          df_NA_test_x[col] = predictions\n",
        "                          tmpdf[col].update(df_NA_test_x[col])\n",
        "                          print(\"INFO : \" + str(\n",
        "                              datetime.now()) + ' : ' + 'Column ' + col + ',data imputing completed : No. of data imputed = ' + str(\n",
        "                              df_NA_test_x.shape[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return model_null_impute_notcat_dt_lvl\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def model_null_impute_notcat_rf(self):\n",
        "      def model_null_impute_notcat_rf_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  column_list = self.noncatcols_list\n",
        "                  for col in column_list:\n",
        "                      #df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y = self.testtrainna(tmpdf, col)\n",
        "                      df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y = self.testtrainna(tmpdf, col)\n",
        "                      if df_notNA_train_x.empty:\n",
        "                          pass\n",
        "                          #print(\"INFO : \" + str(datetime.now()) + ' : ' + 'imputing cancelled')\n",
        "                      else:\n",
        "                          rfc = RandomForestRegressor()\n",
        "                          rfc.fit(df_notNA_train_x, df_notNA_train_y)\n",
        "                          predictions = rfc.predict(df_NA_test_x)\n",
        "                          df_NA_test_x[col] = predictions\n",
        "                          tmpdf[col].update(df_NA_test_x[col])\n",
        "                          print(\"INFO : \" + str(\n",
        "                              datetime.now()) + ' : ' + 'Column ' + col + ',data imputing completed : No. of data imputed = ' + str(\n",
        "                              df_NA_test_x.shape[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return model_null_impute_notcat_rf_lvl\n",
        "\n",
        "    def model_null_impute_cat_rf(self):\n",
        "\n",
        "      def model_null_impute_cat_rf_lvl(func):\n",
        "          def wrapper(tmpdf):\n",
        "              try:\n",
        "                  tmpdf = func(tmpdf)\n",
        "                  self.refresh_cat_noncat_cols_fn(tmpdf,self.threshold)\n",
        "                  column_list = self.catcols_list\n",
        "                  for col in column_list:\n",
        "                      df_notNA_train_x, df_notNA_train_y, df_NA_test_x, df_NA_test_y = self.testtrainna(tmpdf, col)\n",
        "                      if df_notNA_train_x.empty:\n",
        "                          pass\n",
        "                          #print(\"INFO : \" + str(datetime.now()) + ' : ' + 'imputing cancelled')\n",
        "                      else:\n",
        "                          rfc = RandomForestClassifier()\n",
        "                          rfc.fit(df_notNA_train_x, df_notNA_train_y)\n",
        "                          predictions = rfc.predict(df_NA_test_x)\n",
        "                          df_NA_test_x[col] = predictions\n",
        "                          tmpdf[col].update(df_NA_test_x[col])\n",
        "                          print(\"INFO : \" + str(\n",
        "                              datetime.now()) + ' : ' + 'Column ' + col + ',data imputing completed : No. of data imputed = ' + str(\n",
        "                              df_NA_test_x.shape[0]))\n",
        "                  return tmpdf\n",
        "              except:\n",
        "                  sys.exit('ERROR : ' + str(datetime.now()) + ' : ' + sys.exc_info()[1])\n",
        "\n",
        "          return wrapper\n",
        "\n",
        "      return model_null_impute_cat_rf_lvl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj9VZJNyNIPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dc = datacleaner(filename,targetcol='y',cat_threshold=150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VROGaIR2MO05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dc.model_null_impute_cat_rf()\n",
        "@dc.model_null_impute_notcat_rf()\n",
        "@dc.retail_reject_cols(threshold=0.4)\n",
        "def clean_data(tmpdf):\n",
        "  return tmpdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBVrHZe-CFCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4284bb5e-80d0-4f79-b068-4e59315d2646"
      },
      "source": [
        "a = 'True'\n",
        "t = bool(a)\n",
        "type(t)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYMwDCnMYtR",
        "colab_type": "code",
        "outputId": "b126c131-72e8-4bcb-945f-1c29bd3c288c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cleandf = clean_data(dc.df_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rejected columns 23\n",
            "Number of retained columns 282\n",
            "INFO : 2019-08-16 21:07:12.902403 : Column x001has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:13.894577 : Column x002,data imputing completed : No. of data imputed = 21432\n",
            "INFO : 2019-08-16 21:07:14.724998 : Column x003,data imputing completed : No. of data imputed = 21432\n",
            "INFO : 2019-08-16 21:07:15.565748 : Column x004,data imputing completed : No. of data imputed = 21424\n",
            "INFO : 2019-08-16 21:07:16.431398 : Column x005,data imputing completed : No. of data imputed = 6110\n",
            "INFO : 2019-08-16 21:07:17.394329 : Column x041,data imputing completed : No. of data imputed = 36872\n",
            "INFO : 2019-08-16 21:07:17.397614 : Column x042has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:17.407049 : Column x043has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:18.282872 : Column x044,data imputing completed : No. of data imputed = 19674\n",
            "INFO : 2019-08-16 21:07:19.133283 : Column x045,data imputing completed : No. of data imputed = 19674\n",
            "INFO : 2019-08-16 21:07:20.023915 : Column x057,data imputing completed : No. of data imputed = 36872\n",
            "INFO : 2019-08-16 21:07:20.901970 : Column x058,data imputing completed : No. of data imputed = 36872\n",
            "INFO : 2019-08-16 21:07:20.905335 : Column x075has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.068507 : Column x098,data imputing completed : No. of data imputed = 80681\n",
            "INFO : 2019-08-16 21:07:22.071768 : Column x111has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.080329 : Column x120has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.083170 : Column x121has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.085858 : Column x130has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.088591 : Column x131has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.091339 : Column x140has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.094034 : Column x141has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:22.096658 : Column x146has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:23.009378 : Column x148,data imputing completed : No. of data imputed = 41785\n",
            "INFO : 2019-08-16 21:07:24.185500 : Column x155,data imputing completed : No. of data imputed = 79051\n",
            "INFO : 2019-08-16 21:07:25.310633 : Column x162,data imputing completed : No. of data imputed = 66481\n",
            "INFO : 2019-08-16 21:07:25.314121 : Column x186has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.325898 : Column x194has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.328714 : Column x195has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.331637 : Column x196has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.334548 : Column x204has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.337475 : Column x205has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.340392 : Column x206has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.343562 : Column x214has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.346581 : Column x215has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.349514 : Column x216has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.352480 : Column x220has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:25.355354 : Column x221has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:26.386412 : Column x222,data imputing completed : No. of data imputed = 36987\n",
            "INFO : 2019-08-16 21:07:27.399927 : Column x223,data imputing completed : No. of data imputed = 37069\n",
            "INFO : 2019-08-16 21:07:27.403234 : Column x233has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:28.369522 : Column x234,data imputing completed : No. of data imputed = 19110\n",
            "INFO : 2019-08-16 21:07:29.373466 : Column x235,data imputing completed : No. of data imputed = 20083\n",
            "INFO : 2019-08-16 21:07:29.376925 : Column x236has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:30.458745 : Column x237,data imputing completed : No. of data imputed = 36744\n",
            "INFO : 2019-08-16 21:07:31.503082 : Column x238,data imputing completed : No. of data imputed = 36744\n",
            "INFO : 2019-08-16 21:07:32.491037 : Column x239,data imputing completed : No. of data imputed = 36744\n",
            "INFO : 2019-08-16 21:07:33.811244 : Column x242,data imputing completed : No. of data imputed = 93339\n",
            "INFO : 2019-08-16 21:07:33.814796 : Column x243has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:35.088258 : Column x253,data imputing completed : No. of data imputed = 66333\n",
            "INFO : 2019-08-16 21:07:36.594452 : Column x255,data imputing completed : No. of data imputed = 76913\n",
            "INFO : 2019-08-16 21:07:37.959431 : Column x256,data imputing completed : No. of data imputed = 76913\n",
            "INFO : 2019-08-16 21:07:39.337083 : Column x257,data imputing completed : No. of data imputed = 76913\n",
            "INFO : 2019-08-16 21:07:39.340324 : Column x258has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:40.663648 : Column x259,data imputing completed : No. of data imputed = 77432\n",
            "INFO : 2019-08-16 21:07:40.666938 : Column x264has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:44.084543 : Column x265,data imputing completed : No. of data imputed = 66461\n",
            "INFO : 2019-08-16 21:07:46.628098 : Column x266,data imputing completed : No. of data imputed = 66461\n",
            "INFO : 2019-08-16 21:07:48.865412 : Column x267,data imputing completed : No. of data imputed = 66461\n",
            "INFO : 2019-08-16 21:07:51.961905 : Column x268,data imputing completed : No. of data imputed = 67253\n",
            "INFO : 2019-08-16 21:07:55.687090 : Column x272,data imputing completed : No. of data imputed = 7189\n",
            "INFO : 2019-08-16 21:07:55.690305 : Column x273has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:55.703141 : Column x274has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:59.942997 : Column x275,data imputing completed : No. of data imputed = 56131\n",
            "INFO : 2019-08-16 21:07:59.946303 : Column x279has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:59.958958 : Column x280has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:07:59.961606 : Column x281has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:08:03.534830 : Column x287,data imputing completed : No. of data imputed = 24821\n",
            "INFO : 2019-08-16 21:08:07.398784 : Column x288,data imputing completed : No. of data imputed = 49756\n",
            "INFO : 2019-08-16 21:08:11.216351 : Column x289,data imputing completed : No. of data imputed = 49756\n",
            "INFO : 2019-08-16 21:08:15.842568 : Column x290,data imputing completed : No. of data imputed = 49756\n",
            "INFO : 2019-08-16 21:08:15.846234 : Column x291has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:08:15.861215 : Column x292has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:08:20.802882 : Column x293,data imputing completed : No. of data imputed = 51133\n",
            "INFO : 2019-08-16 21:08:20.806014 : Column x294has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:08:26.045448 : Column x295,data imputing completed : No. of data imputed = 86533\n",
            "INFO : 2019-08-16 21:08:26.048865 : Column x296has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:08:39.139270 : Column x297,data imputing completed : No. of data imputed = 58112\n",
            "INFO : 2019-08-16 21:08:49.807913 : Column x302,data imputing completed : No. of data imputed = 73069\n",
            "INFO : 2019-08-16 21:08:49.810914 : Column x303has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.091873 : Column x304,data imputing completed : No. of data imputed = 81875\n",
            "INFO : 2019-08-16 21:09:09.425233 : Column x006has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.428191 : Column x007has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.430934 : Column x008has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.433563 : Column x009has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.436210 : Column x010has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.438820 : Column x011has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.441456 : Column x012has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.444096 : Column x013has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.446700 : Column x014has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.449327 : Column x015has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.451923 : Column x016has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.454545 : Column x017has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.457166 : Column x018has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.459760 : Column x019has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.462458 : Column x020has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.465082 : Column x021has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.467677 : Column x022has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.470313 : Column x023has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.472907 : Column x024has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.475563 : Column x025has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.478180 : Column x026has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.480812 : Column x027has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.483519 : Column x028has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.486133 : Column x029has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.488722 : Column x030has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.491343 : Column x031has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.493973 : Column x032has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.496602 : Column x033has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.499276 : Column x034has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.502000 : Column x035has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.504590 : Column x036has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.507349 : Column x037has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.510058 : Column x038has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.512813 : Column x039has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.515446 : Column x040has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.518195 : Column x046has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.520931 : Column x047has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.523548 : Column x048has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.526399 : Column x049has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.529210 : Column x050has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.531808 : Column x051has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.534433 : Column x052has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.537059 : Column x053has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.539652 : Column x054has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.542307 : Column x055has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.544969 : Column x056has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.547562 : Column x059has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.550176 : Column x060has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.552935 : Column x061has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.555695 : Column x062has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.558319 : Column x063has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.560899 : Column x064has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.563526 : Column x065has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.566161 : Column x066has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.568792 : Column x067has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.571415 : Column x068has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.574036 : Column x069has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.576652 : Column x070has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.579385 : Column x071has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.581993 : Column x072has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.584590 : Column x073has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.587340 : Column x074has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.589921 : Column x076has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.592550 : Column x077has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.595180 : Column x078has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.597788 : Column x079has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.600410 : Column x080has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.603006 : Column x081has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.605600 : Column x082has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.608204 : Column x083has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.610812 : Column x084has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.613427 : Column x085has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.616043 : Column x086has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.618647 : Column x087has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.621277 : Column x088has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.623880 : Column x089has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.626490 : Column x090has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.630369 : Column x091has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.633354 : Column x092has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.636237 : Column x093has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.638893 : Column x094has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.641716 : Column x095has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.644380 : Column x096has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.646939 : Column x097has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.649501 : Column x099has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.652087 : Column x100has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.654629 : Column x101has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.657231 : Column x102has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.659856 : Column x103has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.662468 : Column x104has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.665088 : Column x105has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.667724 : Column x106has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.670339 : Column x107has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.672888 : Column x108has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.675463 : Column x109has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.678043 : Column x110has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.680600 : Column x112has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.683197 : Column x113has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.685738 : Column x114has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.688418 : Column x115has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.691015 : Column x116has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.693573 : Column x117has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.696149 : Column x118has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.698701 : Column x119has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.701284 : Column x122has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.703842 : Column x123has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.706395 : Column x124has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.708929 : Column x125has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.711492 : Column x126has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.714135 : Column x127has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.716750 : Column x128has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.719317 : Column x129has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.721973 : Column x132has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.724569 : Column x133has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.727196 : Column x134has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.729770 : Column x135has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.732373 : Column x136has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.735415 : Column x137has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.738011 : Column x138has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.740595 : Column x139has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.743180 : Column x142has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.745727 : Column x143has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.748322 : Column x144has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.751081 : Column x145has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.753664 : Column x147has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.756342 : Column x149has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.758981 : Column x150has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.761652 : Column x151has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.764244 : Column x152has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.766873 : Column x153has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.769513 : Column x154has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.772162 : Column x156has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.774755 : Column x157has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.777397 : Column x158has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.779971 : Column x159has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.782527 : Column x160has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.785108 : Column x161has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.787676 : Column x163has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.790295 : Column x164has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.792872 : Column x165has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.795490 : Column x166has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.798123 : Column x167has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.800750 : Column x168has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.803324 : Column x169has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.805912 : Column x170has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.808489 : Column x171has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.811052 : Column x172has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.813621 : Column x173has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.816224 : Column x174has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.818786 : Column x175has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.821338 : Column x176has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.823889 : Column x177has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.826597 : Column x178has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.829153 : Column x179has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.832338 : Column x180has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.835342 : Column x181has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.838599 : Column x182has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.841345 : Column x183has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.844080 : Column x184has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.846757 : Column x185has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.849442 : Column x187has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.852110 : Column x188has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.854734 : Column x189has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.857409 : Column x190has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.860065 : Column x191has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.862724 : Column x192has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.865541 : Column x193has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.868206 : Column x197has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.870843 : Column x198has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.873590 : Column x199has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.876539 : Column x200has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.879339 : Column x201has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.882023 : Column x202has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.884687 : Column x203has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.887290 : Column x207has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.889874 : Column x208has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.892522 : Column x209has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.895262 : Column x210has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.897894 : Column x211has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.900521 : Column x212has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.903159 : Column x213has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.905874 : Column x217has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.908624 : Column x218has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.911235 : Column x219has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.913810 : Column x224has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.916424 : Column x225has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.919013 : Column x226has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.921619 : Column x227has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.924226 : Column x228has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.926801 : Column x229has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.929407 : Column x230has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.932131 : Column x231has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.934795 : Column x232has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.937430 : Column x240has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.940038 : Column x241has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.942688 : Column x244has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.945298 : Column x245has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.947919 : Column x246has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.950545 : Column x247has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.953220 : Column x248has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.955826 : Column x249has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.958429 : Column x250has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.961033 : Column x251has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.963619 : Column x252has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.966224 : Column x254has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.968961 : Column x260has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.971595 : Column x261has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.974415 : Column x262has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.977084 : Column x263has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.979672 : Column x269has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.982278 : Column x270has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.984852 : Column x271has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.987492 : Column x276has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.990345 : Column x277has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.992929 : Column x278has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.995549 : Column x282has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:09.998151 : Column x283has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.000755 : Column x284has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.003617 : Column x285has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.006301 : Column x286has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.008902 : Column x298has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.011560 : Column x299has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.014298 : Column x300has no null values so imputing cancelled\n",
            "INFO : 2019-08-16 21:09:10.016967 : Column x301has no null values so imputing cancelled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol7oArEVL963",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_imp_list,f_impdf = dc.importantfeatures(cleandf,tobepredicted='y',modelname='lgbmregressor',skipcols=[],featurelimit=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ExtNDl1U5oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_columns = f_impdf[:200]['feature'].tolist() + ['y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJxqaBErzYZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleandf = cleandf[important_columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCCjjzTCw5HO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@dc.refresh_cat_noncat_cols(threshold=100)\n",
        "#@dc.standardize_simple_auto(range_tuple = (0,10))\n",
        "#@dc.refresh_cat_noncat_cols(threshold=200)\n",
        "@dc.ohe_on_column(drop_converted_col=True)\n",
        "@dc.refresh_cat_noncat_cols(threshold=150)\n",
        "#@dc.featurization(cat_coltype=False)\n",
        "#@dc.two_column_featurization(cat_coltype=False)\n",
        "#@dc.refresh_cat_noncat_cols(threshold=100)\n",
        "#@dc.convertdatatypes(cat_threshold=150)\n",
        "#@dc.applypca(cat_coltype=False ,number_of_components = 10)\n",
        "#@dc.refresh_cat_noncat_cols(threshold=100)\n",
        "#@dc.high_coor_target_column(targetcol = 'y',th=0.05)\n",
        "#@dc.remove_collinear(th=0.95)\n",
        "#@dc.model_null_impute_cat_rf()\n",
        "#@dc.model_null_impute_notcat_rf()\n",
        "#@dc.refresh_cat_noncat_cols(threshold=100)\n",
        "@dc.convertdatatypes(cat_threshold=150)\n",
        "#@dc.retail_reject_cols(threshold=0.4)\n",
        "def cleandata_lgbm(tmpdf):\n",
        "  return tmpdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5CIUti_TQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm9CEL2GgUkX",
        "colab_type": "code",
        "outputId": "92804891-60c1-4018-8e9c-c3b6552b06cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cleandf = cleandata_lgbm(cleandf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : 2019-08-16 21:10:00.971749 : Size of dataset before applying converdatatypes 244000080\n",
            "INFO : 2019-08-16 21:10:12.426474 : Size of dataset after applying converdatatypes 122000080\n",
            "ref80\n",
            "ref['x001', 'x002', 'x003', 'x004', 'x005', 'x041', 'x042', 'x043', 'x044', 'x045', 'x057', 'x058', 'x075', 'x098', 'x111', 'x120', 'x121', 'x130', 'x131', 'x140', 'x141', 'x146', 'x148', 'x155', 'x162', 'x186', 'x194', 'x195', 'x196', 'x204', 'x205', 'x206', 'x214', 'x215', 'x216', 'x220', 'x221', 'x222', 'x223', 'x233', 'x234', 'x235', 'x236', 'x237', 'x238', 'x239', 'x242', 'x243', 'x253', 'x255', 'x256', 'x257', 'x258', 'x259', 'x264', 'x265', 'x266', 'x267', 'x268', 'x272', 'x273', 'x274', 'x275', 'x279', 'x280', 'x281', 'x287', 'x288', 'x289', 'x290', 'x291', 'x292', 'x293', 'x294', 'x295', 'x296', 'x297', 'x302', 'x303', 'x304']\n",
            "INFO : 2019-08-16 21:10:13.120196 : Column x006 converted to dummies\n",
            "INFO : 2019-08-16 21:10:13.328300 : Column x007 converted to dummies\n",
            "INFO : 2019-08-16 21:10:13.547738 : Column x008 converted to dummies\n",
            "INFO : 2019-08-16 21:10:13.771631 : Column x009 converted to dummies\n",
            "INFO : 2019-08-16 21:10:14.013832 : Column x010 converted to dummies\n",
            "INFO : 2019-08-16 21:10:14.276073 : Column x011 converted to dummies\n",
            "INFO : 2019-08-16 21:10:14.550177 : Column x012 converted to dummies\n",
            "INFO : 2019-08-16 21:10:14.821108 : Column x013 converted to dummies\n",
            "INFO : 2019-08-16 21:10:15.109082 : Column x014 converted to dummies\n",
            "INFO : 2019-08-16 21:10:15.457705 : Column x015 converted to dummies\n",
            "INFO : 2019-08-16 21:10:15.766966 : Column x016 converted to dummies\n",
            "INFO : 2019-08-16 21:10:16.105314 : Column x017 converted to dummies\n",
            "INFO : 2019-08-16 21:10:16.400228 : Column x018 converted to dummies\n",
            "INFO : 2019-08-16 21:10:16.706023 : Column x019 converted to dummies\n",
            "INFO : 2019-08-16 21:10:17.072868 : Column x020 converted to dummies\n",
            "INFO : 2019-08-16 21:10:17.413055 : Column x021 converted to dummies\n",
            "INFO : 2019-08-16 21:10:17.741827 : Column x022 converted to dummies\n",
            "INFO : 2019-08-16 21:10:18.105895 : Column x023 converted to dummies\n",
            "INFO : 2019-08-16 21:10:18.534346 : Column x024 converted to dummies\n",
            "INFO : 2019-08-16 21:10:18.913547 : Column x025 converted to dummies\n",
            "INFO : 2019-08-16 21:10:19.266450 : Column x026 converted to dummies\n",
            "INFO : 2019-08-16 21:10:19.604952 : Column x027 converted to dummies\n",
            "INFO : 2019-08-16 21:10:19.967387 : Column x028 converted to dummies\n",
            "INFO : 2019-08-16 21:10:20.332343 : Column x029 converted to dummies\n",
            "INFO : 2019-08-16 21:10:20.719825 : Column x030 converted to dummies\n",
            "INFO : 2019-08-16 21:10:21.175467 : Column x031 converted to dummies\n",
            "INFO : 2019-08-16 21:10:21.592469 : Column x032 converted to dummies\n",
            "INFO : 2019-08-16 21:10:22.001278 : Column x033 converted to dummies\n",
            "INFO : 2019-08-16 21:10:22.420745 : Column x034 converted to dummies\n",
            "INFO : 2019-08-16 21:10:22.872759 : Column x035 converted to dummies\n",
            "INFO : 2019-08-16 21:10:23.354411 : Column x036 converted to dummies\n",
            "INFO : 2019-08-16 21:10:23.833345 : Column x037 converted to dummies\n",
            "INFO : 2019-08-16 21:10:24.318393 : Column x038 converted to dummies\n",
            "INFO : 2019-08-16 21:10:24.805613 : Column x039 converted to dummies\n",
            "INFO : 2019-08-16 21:10:25.298462 : Column x040 converted to dummies\n",
            "INFO : 2019-08-16 21:10:25.757926 : Column x046 converted to dummies\n",
            "INFO : 2019-08-16 21:10:26.207359 : Column x047 converted to dummies\n",
            "INFO : 2019-08-16 21:10:26.650217 : Column x048 converted to dummies\n",
            "INFO : 2019-08-16 21:10:27.097657 : Column x049 converted to dummies\n",
            "INFO : 2019-08-16 21:10:27.526828 : Column x050 converted to dummies\n",
            "INFO : 2019-08-16 21:10:27.961603 : Column x051 converted to dummies\n",
            "INFO : 2019-08-16 21:10:28.400993 : Column x052 converted to dummies\n",
            "INFO : 2019-08-16 21:10:28.849212 : Column x053 converted to dummies\n",
            "INFO : 2019-08-16 21:10:29.299161 : Column x054 converted to dummies\n",
            "INFO : 2019-08-16 21:10:29.759541 : Column x055 converted to dummies\n",
            "INFO : 2019-08-16 21:10:30.218000 : Column x056 converted to dummies\n",
            "INFO : 2019-08-16 21:10:30.722290 : Column x059 converted to dummies\n",
            "INFO : 2019-08-16 21:10:31.190867 : Column x060 converted to dummies\n",
            "INFO : 2019-08-16 21:10:31.660734 : Column x061 converted to dummies\n",
            "INFO : 2019-08-16 21:10:32.131074 : Column x062 converted to dummies\n",
            "INFO : 2019-08-16 21:10:32.624281 : Column x063 converted to dummies\n",
            "INFO : 2019-08-16 21:10:33.111351 : Column x064 converted to dummies\n",
            "INFO : 2019-08-16 21:10:33.613137 : Column x065 converted to dummies\n",
            "INFO : 2019-08-16 21:10:34.131282 : Column x066 converted to dummies\n",
            "INFO : 2019-08-16 21:10:34.662294 : Column x067 converted to dummies\n",
            "INFO : 2019-08-16 21:10:35.216622 : Column x068 converted to dummies\n",
            "INFO : 2019-08-16 21:10:35.792887 : Column x069 converted to dummies\n",
            "INFO : 2019-08-16 21:10:36.361135 : Column x070 converted to dummies\n",
            "INFO : 2019-08-16 21:10:36.921790 : Column x071 converted to dummies\n",
            "INFO : 2019-08-16 21:10:37.447738 : Column x072 converted to dummies\n",
            "INFO : 2019-08-16 21:10:38.025331 : Column x073 converted to dummies\n",
            "INFO : 2019-08-16 21:10:38.625242 : Column x074 converted to dummies\n",
            "INFO : 2019-08-16 21:10:39.233052 : Column x076 converted to dummies\n",
            "INFO : 2019-08-16 21:10:39.834628 : Column x077 converted to dummies\n",
            "INFO : 2019-08-16 21:10:40.439832 : Column x078 converted to dummies\n",
            "INFO : 2019-08-16 21:10:41.030636 : Column x079 converted to dummies\n",
            "INFO : 2019-08-16 21:10:41.614888 : Column x080 converted to dummies\n",
            "INFO : 2019-08-16 21:10:42.222123 : Column x081 converted to dummies\n",
            "INFO : 2019-08-16 21:10:42.821455 : Column x082 converted to dummies\n",
            "INFO : 2019-08-16 21:10:43.439918 : Column x083 converted to dummies\n",
            "INFO : 2019-08-16 21:10:44.027271 : Column x084 converted to dummies\n",
            "INFO : 2019-08-16 21:10:44.594933 : Column x085 converted to dummies\n",
            "INFO : 2019-08-16 21:10:45.191828 : Column x086 converted to dummies\n",
            "INFO : 2019-08-16 21:10:45.741897 : Column x087 converted to dummies\n",
            "INFO : 2019-08-16 21:10:46.292890 : Column x088 converted to dummies\n",
            "INFO : 2019-08-16 21:10:46.840386 : Column x089 converted to dummies\n",
            "INFO : 2019-08-16 21:10:47.389922 : Column x090 converted to dummies\n",
            "INFO : 2019-08-16 21:10:47.949557 : Column x091 converted to dummies\n",
            "INFO : 2019-08-16 21:10:48.553821 : Column x092 converted to dummies\n",
            "INFO : 2019-08-16 21:10:49.164671 : Column x093 converted to dummies\n",
            "INFO : 2019-08-16 21:10:49.767501 : Column x094 converted to dummies\n",
            "INFO : 2019-08-16 21:10:50.329028 : Column x095 converted to dummies\n",
            "INFO : 2019-08-16 21:10:50.933985 : Column x096 converted to dummies\n",
            "INFO : 2019-08-16 21:10:51.526218 : Column x097 converted to dummies\n",
            "INFO : 2019-08-16 21:10:52.104363 : Column x099 converted to dummies\n",
            "INFO : 2019-08-16 21:10:52.745673 : Column x100 converted to dummies\n",
            "INFO : 2019-08-16 21:10:53.362869 : Column x101 converted to dummies\n",
            "INFO : 2019-08-16 21:10:53.973000 : Column x102 converted to dummies\n",
            "INFO : 2019-08-16 21:10:54.579991 : Column x103 converted to dummies\n",
            "INFO : 2019-08-16 21:10:55.174225 : Column x104 converted to dummies\n",
            "INFO : 2019-08-16 21:10:55.775646 : Column x105 converted to dummies\n",
            "INFO : 2019-08-16 21:10:56.429877 : Column x106 converted to dummies\n",
            "INFO : 2019-08-16 21:10:57.084817 : Column x107 converted to dummies\n",
            "INFO : 2019-08-16 21:10:57.728967 : Column x108 converted to dummies\n",
            "INFO : 2019-08-16 21:10:58.325320 : Column x109 converted to dummies\n",
            "INFO : 2019-08-16 21:10:58.921832 : Column x110 converted to dummies\n",
            "INFO : 2019-08-16 21:10:59.531345 : Column x112 converted to dummies\n",
            "INFO : 2019-08-16 21:11:00.163651 : Column x113 converted to dummies\n",
            "INFO : 2019-08-16 21:11:00.815817 : Column x114 converted to dummies\n",
            "INFO : 2019-08-16 21:11:01.500853 : Column x115 converted to dummies\n",
            "INFO : 2019-08-16 21:11:02.210255 : Column x116 converted to dummies\n",
            "INFO : 2019-08-16 21:11:02.903138 : Column x117 converted to dummies\n",
            "INFO : 2019-08-16 21:11:03.564957 : Column x118 converted to dummies\n",
            "INFO : 2019-08-16 21:11:04.276546 : Column x119 converted to dummies\n",
            "INFO : 2019-08-16 21:11:04.953219 : Column x122 converted to dummies\n",
            "INFO : 2019-08-16 21:11:05.715849 : Column x123 converted to dummies\n",
            "INFO : 2019-08-16 21:11:06.435990 : Column x124 converted to dummies\n",
            "INFO : 2019-08-16 21:11:07.164709 : Column x125 converted to dummies\n",
            "INFO : 2019-08-16 21:11:07.922182 : Column x126 converted to dummies\n",
            "INFO : 2019-08-16 21:11:08.648953 : Column x127 converted to dummies\n",
            "INFO : 2019-08-16 21:11:09.414319 : Column x128 converted to dummies\n",
            "INFO : 2019-08-16 21:11:10.209526 : Column x129 converted to dummies\n",
            "INFO : 2019-08-16 21:11:10.982345 : Column x132 converted to dummies\n",
            "INFO : 2019-08-16 21:11:11.811253 : Column x133 converted to dummies\n",
            "INFO : 2019-08-16 21:11:12.671734 : Column x134 converted to dummies\n",
            "INFO : 2019-08-16 21:11:13.552772 : Column x135 converted to dummies\n",
            "INFO : 2019-08-16 21:11:14.379834 : Column x136 converted to dummies\n",
            "INFO : 2019-08-16 21:11:15.197676 : Column x137 converted to dummies\n",
            "INFO : 2019-08-16 21:11:16.055029 : Column x138 converted to dummies\n",
            "INFO : 2019-08-16 21:11:16.971554 : Column x139 converted to dummies\n",
            "INFO : 2019-08-16 21:11:17.840329 : Column x142 converted to dummies\n",
            "INFO : 2019-08-16 21:11:18.683585 : Column x143 converted to dummies\n",
            "INFO : 2019-08-16 21:11:19.570535 : Column x144 converted to dummies\n",
            "INFO : 2019-08-16 21:11:20.566129 : Column x145 converted to dummies\n",
            "INFO : 2019-08-16 21:11:21.553632 : Column x147 converted to dummies\n",
            "INFO : 2019-08-16 21:11:22.530054 : Column x149 converted to dummies\n",
            "INFO : 2019-08-16 21:11:23.540884 : Column x150 converted to dummies\n",
            "INFO : 2019-08-16 21:11:24.551263 : Column x151 converted to dummies\n",
            "INFO : 2019-08-16 21:11:25.462454 : Column x152 converted to dummies\n",
            "INFO : 2019-08-16 21:11:26.370037 : Column x153 converted to dummies\n",
            "INFO : 2019-08-16 21:11:27.260087 : Column x154 converted to dummies\n",
            "INFO : 2019-08-16 21:11:28.134364 : Column x156 converted to dummies\n",
            "INFO : 2019-08-16 21:11:29.085201 : Column x157 converted to dummies\n",
            "INFO : 2019-08-16 21:11:30.130554 : Column x158 converted to dummies\n",
            "INFO : 2019-08-16 21:11:31.118566 : Column x159 converted to dummies\n",
            "INFO : 2019-08-16 21:11:32.104148 : Column x160 converted to dummies\n",
            "INFO : 2019-08-16 21:11:33.125190 : Column x161 converted to dummies\n",
            "INFO : 2019-08-16 21:11:34.079552 : Column x163 converted to dummies\n",
            "INFO : 2019-08-16 21:11:35.056061 : Column x164 converted to dummies\n",
            "INFO : 2019-08-16 21:11:36.105404 : Column x165 converted to dummies\n",
            "INFO : 2019-08-16 21:11:37.162092 : Column x166 converted to dummies\n",
            "INFO : 2019-08-16 21:11:38.196216 : Column x167 converted to dummies\n",
            "INFO : 2019-08-16 21:11:39.207607 : Column x168 converted to dummies\n",
            "INFO : 2019-08-16 21:11:40.264433 : Column x169 converted to dummies\n",
            "INFO : 2019-08-16 21:11:41.357112 : Column x170 converted to dummies\n",
            "INFO : 2019-08-16 21:11:42.474323 : Column x171 converted to dummies\n",
            "INFO : 2019-08-16 21:11:43.568793 : Column x172 converted to dummies\n",
            "INFO : 2019-08-16 21:11:44.634281 : Column x173 converted to dummies\n",
            "INFO : 2019-08-16 21:11:45.710266 : Column x174 converted to dummies\n",
            "INFO : 2019-08-16 21:11:46.755934 : Column x175 converted to dummies\n",
            "INFO : 2019-08-16 21:11:47.900198 : Column x176 converted to dummies\n",
            "INFO : 2019-08-16 21:11:49.034095 : Column x177 converted to dummies\n",
            "INFO : 2019-08-16 21:11:50.160507 : Column x178 converted to dummies\n",
            "INFO : 2019-08-16 21:11:51.296920 : Column x179 converted to dummies\n",
            "INFO : 2019-08-16 21:11:52.417386 : Column x180 converted to dummies\n",
            "INFO : 2019-08-16 21:11:53.525429 : Column x181 converted to dummies\n",
            "INFO : 2019-08-16 21:11:54.556501 : Column x182 converted to dummies\n",
            "INFO : 2019-08-16 21:11:55.586140 : Column x183 converted to dummies\n",
            "INFO : 2019-08-16 21:11:56.654036 : Column x184 converted to dummies\n",
            "INFO : 2019-08-16 21:11:57.695143 : Column x185 converted to dummies\n",
            "INFO : 2019-08-16 21:11:58.720283 : Column x187 converted to dummies\n",
            "INFO : 2019-08-16 21:11:59.849595 : Column x188 converted to dummies\n",
            "INFO : 2019-08-16 21:12:00.932961 : Column x189 converted to dummies\n",
            "INFO : 2019-08-16 21:12:02.034275 : Column x190 converted to dummies\n",
            "INFO : 2019-08-16 21:12:03.188283 : Column x191 converted to dummies\n",
            "INFO : 2019-08-16 21:12:04.315029 : Column x192 converted to dummies\n",
            "INFO : 2019-08-16 21:12:05.442518 : Column x193 converted to dummies\n",
            "INFO : 2019-08-16 21:12:06.511822 : Column x197 converted to dummies\n",
            "INFO : 2019-08-16 21:12:07.699555 : Column x198 converted to dummies\n",
            "INFO : 2019-08-16 21:12:08.908796 : Column x199 converted to dummies\n",
            "INFO : 2019-08-16 21:12:10.024106 : Column x200 converted to dummies\n",
            "INFO : 2019-08-16 21:12:11.261461 : Column x201 converted to dummies\n",
            "INFO : 2019-08-16 21:12:12.557410 : Column x202 converted to dummies\n",
            "INFO : 2019-08-16 21:12:13.754344 : Column x203 converted to dummies\n",
            "INFO : 2019-08-16 21:12:14.924349 : Column x207 converted to dummies\n",
            "INFO : 2019-08-16 21:12:16.228575 : Column x208 converted to dummies\n",
            "INFO : 2019-08-16 21:12:17.506818 : Column x209 converted to dummies\n",
            "INFO : 2019-08-16 21:12:18.847426 : Column x210 converted to dummies\n",
            "INFO : 2019-08-16 21:12:20.066265 : Column x211 converted to dummies\n",
            "INFO : 2019-08-16 21:12:21.310542 : Column x212 converted to dummies\n",
            "INFO : 2019-08-16 21:12:22.642828 : Column x213 converted to dummies\n",
            "INFO : 2019-08-16 21:12:23.995027 : Column x217 converted to dummies\n",
            "INFO : 2019-08-16 21:12:25.426117 : Column x218 converted to dummies\n",
            "INFO : 2019-08-16 21:12:26.860263 : Column x219 converted to dummies\n",
            "INFO : 2019-08-16 21:12:28.280490 : Column x224 converted to dummies\n",
            "INFO : 2019-08-16 21:12:29.736431 : Column x225 converted to dummies\n",
            "INFO : 2019-08-16 21:12:31.173556 : Column x226 converted to dummies\n",
            "INFO : 2019-08-16 21:12:32.604343 : Column x227 converted to dummies\n",
            "INFO : 2019-08-16 21:12:33.979726 : Column x228 converted to dummies\n",
            "INFO : 2019-08-16 21:12:35.250605 : Column x229 converted to dummies\n",
            "INFO : 2019-08-16 21:12:36.635591 : Column x230 converted to dummies\n",
            "INFO : 2019-08-16 21:12:38.115449 : Column x231 converted to dummies\n",
            "INFO : 2019-08-16 21:12:39.593436 : Column x232 converted to dummies\n",
            "INFO : 2019-08-16 21:12:41.110900 : Column x240 converted to dummies\n",
            "INFO : 2019-08-16 21:12:42.617163 : Column x241 converted to dummies\n",
            "INFO : 2019-08-16 21:12:44.112357 : Column x244 converted to dummies\n",
            "INFO : 2019-08-16 21:12:45.541239 : Column x245 converted to dummies\n",
            "INFO : 2019-08-16 21:12:46.913024 : Column x246 converted to dummies\n",
            "INFO : 2019-08-16 21:12:48.447816 : Column x247 converted to dummies\n",
            "INFO : 2019-08-16 21:12:49.804256 : Column x248 converted to dummies\n",
            "INFO : 2019-08-16 21:12:51.220011 : Column x249 converted to dummies\n",
            "INFO : 2019-08-16 21:12:52.705622 : Column x250 converted to dummies\n",
            "INFO : 2019-08-16 21:12:54.014278 : Column x251 converted to dummies\n",
            "INFO : 2019-08-16 21:12:55.301860 : Column x252 converted to dummies\n",
            "INFO : 2019-08-16 21:12:56.631578 : Column x254 converted to dummies\n",
            "INFO : 2019-08-16 21:12:57.993546 : Column x260 converted to dummies\n",
            "INFO : 2019-08-16 21:12:59.435806 : Column x261 converted to dummies\n",
            "INFO : 2019-08-16 21:13:00.917964 : Column x262 converted to dummies\n",
            "INFO : 2019-08-16 21:13:02.274190 : Column x263 converted to dummies\n",
            "INFO : 2019-08-16 21:13:03.609125 : Column x269 converted to dummies\n",
            "INFO : 2019-08-16 21:13:04.958565 : Column x270 converted to dummies\n",
            "INFO : 2019-08-16 21:13:06.305489 : Column x271 converted to dummies\n",
            "INFO : 2019-08-16 21:13:07.673894 : Column x276 converted to dummies\n",
            "INFO : 2019-08-16 21:13:08.994129 : Column x277 converted to dummies\n",
            "INFO : 2019-08-16 21:13:10.249078 : Column x278 converted to dummies\n",
            "INFO : 2019-08-16 21:13:11.627201 : Column x282 converted to dummies\n",
            "INFO : 2019-08-16 21:13:12.931186 : Column x283 converted to dummies\n",
            "INFO : 2019-08-16 21:13:14.256874 : Column x284 converted to dummies\n",
            "INFO : 2019-08-16 21:13:15.642303 : Column x285 converted to dummies\n",
            "INFO : 2019-08-16 21:13:17.122138 : Column x286 converted to dummies\n",
            "INFO : 2019-08-16 21:13:18.642211 : Column x298 converted to dummies\n",
            "INFO : 2019-08-16 21:13:20.118843 : Column x299 converted to dummies\n",
            "INFO : 2019-08-16 21:13:21.487911 : Column x300 converted to dummies\n",
            "INFO : 2019-08-16 21:13:22.946145 : Column x301 converted to dummies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF5pRgnvmsvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "def create_train_test_val_loaders(train,test,val=0.2,batch_size=50):\n",
        "  targetcol = 'y'\n",
        "  train,val = train_test_split(train,test_size=0.2,random_state=0)\n",
        "  train  = train.reset_index(drop=True)\n",
        "  test  = test.reset_index(drop=True)\n",
        "  val  = val.reset_index(drop=True)\n",
        "  feature_cols = [col for col in train.columns.tolist() if col!='y']\n",
        "  #f_tensor = torch.tensor(train[feature_cols].values)\n",
        "  #t_tensor = torch.tensor(train['y'].values)\n",
        "  #RegressionColumnarDataset(df, catcols,targetcol,mode='train',catth=0.2)\n",
        "  train_tensor = data_utils.TensorDataset(torch.tensor(train[feature_cols].values), torch.tensor(train['y'].values))\n",
        "  train_loader = data_utils.DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
        "  val_tensor = data_utils.TensorDataset(torch.tensor(val[feature_cols].values), torch.tensor(val['y'].values))\n",
        "  val_loader = data_utils.DataLoader(val_tensor, batch_size=batch_size, shuffle=True)\n",
        "  test_tensor = data_utils.TensorDataset(torch.tensor(test[feature_cols].values), torch.tensor(test['id'].values))\n",
        "  test_loader = data_utils.DataLoader(test_tensor, batch_size=batch_size, shuffle=True)\n",
        "  return train_loader,val_loader,test_loader\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_ezj2nV5smy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "cleandf['id'] = cleandf.index +1\n",
        "train,test = train_test_split(cleandf,test_size=0.2,random_state=0)\n",
        "train  = train.reset_index(drop=True)\n",
        "test  = test.reset_index(drop=True)\n",
        "train_loader,val_loader,test_loader = create_train_test_val_loaders(train,test,val=0.2,batch_size=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGS6ic2ro2Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression(torch.nn.Module):\n",
        "    def __init__(self, size_list,drops_list, use_bn=True, D_out=1):\n",
        "        super().__init__()\n",
        "        self.use_bn = use_bn\n",
        "        self.linears = torch.nn.ModuleList([torch.nn.Linear(size_list[i], size_list[i+1]) for i in range(len(size_list)-1)])\n",
        "        self.batch1d = torch.nn.ModuleList([torch.nn.BatchNorm1d(sz) for sz in size_list[1:]])\n",
        "        for x in self.linears: \n",
        "          torch.nn.init.kaiming_normal_(x.weight.data)\n",
        "        self.drops = torch.nn.ModuleList([torch.nn.Dropout(drop) for drop in drops_list])\n",
        "        self.output = torch.nn.Linear(size_list[-1], D_out)\n",
        "        torch.nn.init.kaiming_normal_(self.output.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "      for l,d,b in zip(self.linears, self.drops, self.batch1d):\n",
        "        #x = torch.nn.functional.relu(l(x))\n",
        "        x = l(x)\n",
        "        if self.use_bn: \n",
        "          x = b(x)\n",
        "        x = d(x)\n",
        "      x = self.output(x)\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvChTWloqe_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_pytorch(model,criterion,optimizer,training_loader,validation_loader,device,epochs = 10):\n",
        "    running_loss_history = []\n",
        "    running_corrects_history = []\n",
        "    val_running_loss_history = []\n",
        "    val_running_corrects_history = []\n",
        "    #batch = 0\n",
        "    model.to(device)\n",
        "    for e in range(epochs):\n",
        "        batch = 0\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        val_running_loss = 0.0\n",
        "        val_running_corrects = 0.0\n",
        "  \n",
        "        for inputs, labels in training_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            #print(inputs.shape)\n",
        "            labels = labels.to(device)\n",
        "            inputs = inputs.float()\n",
        "            labels = labels.float()\n",
        "            #print(labels.shape)\n",
        "            batch = batch + len(inputs)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            #print(outputs)\n",
        "            #print(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = torch.abs(outputs.squeeze() - labels) <= 3\n",
        "            #t = torch.sum(preds)\n",
        "            #print(preds)\n",
        "            #print(t)\n",
        "            #print(torch.sum(preds))\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(preds)\n",
        "            \n",
        "        else:\n",
        "            valbatch = 0\n",
        "            with torch.no_grad():\n",
        "                for val_inputs, val_labels in validation_loader:\n",
        "                    val_inputs = val_inputs.to(device)\n",
        "                    val_labels = val_labels.to(device)\n",
        "                    val_labels = val_labels.float()\n",
        "                    val_inputs = val_inputs.float()\n",
        "                    valbatch = valbatch + len(val_inputs)\n",
        "                    val_outputs = model(val_inputs)          \n",
        "                    val_loss = criterion(val_outputs, val_labels)\n",
        "                    val_preds = torch.abs(val_outputs.squeeze() - val_labels) <= 3\n",
        "                    val_running_loss += val_loss.item()\n",
        "                    val_running_corrects += torch.sum(val_preds)\n",
        "        epoch_loss = running_loss/len(training_loader)\n",
        "        epoch_acc = running_corrects.float()/ batch\n",
        "        running_loss_history.append(epoch_loss)\n",
        "        running_corrects_history.append(epoch_acc)\n",
        "        val_epoch_loss = val_running_loss/len(validation_loader)\n",
        "        val_epoch_acc = val_running_corrects.float()/ valbatch\n",
        "        val_running_loss_history.append(val_epoch_loss)\n",
        "        val_running_corrects_history.append(val_epoch_acc)\n",
        "        print('epoch :', (e+1))\n",
        "        print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
        "        print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT1FTku1_h8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RMSELoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = torch.nn.MSELoss()\n",
        "        \n",
        "    def forward(self,pred,actual):\n",
        "        return torch.sqrt(self.mse(pred,actual))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjmHSvNU3kSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.01\n",
        "criterion = RMSELoss() \n",
        "\n",
        "inputDim =1\n",
        "outputDim =1\n",
        "inputval = cleandf.shape[1]-1\n",
        "model = LinearRegression(size_list=[inputval,100,30,100,50,10],drops_list=[0.001,0.02,0.04,0.01,0.01], use_bn=True, D_out=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzfQzPK4BL82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-cnApRegLur",
        "colab_type": "code",
        "outputId": "841df0ff-d783-4221-c594-407dbdc5685c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(\n",
              "  (linears): ModuleList(\n",
              "    (0): Linear(in_features=7474, out_features=100, bias=True)\n",
              "    (1): Linear(in_features=100, out_features=30, bias=True)\n",
              "    (2): Linear(in_features=30, out_features=100, bias=True)\n",
              "    (3): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              "  (batch1d): ModuleList(\n",
              "    (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (drops): ModuleList(\n",
              "    (0): Dropout(p=0.001)\n",
              "    (1): Dropout(p=0.02)\n",
              "    (2): Dropout(p=0.04)\n",
              "    (3): Dropout(p=0.01)\n",
              "    (4): Dropout(p=0.01)\n",
              "  )\n",
              "  (output): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAy95wMU3P81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1a02af2-785e-49f5-b871-9b61158b6647"
      },
      "source": [
        "model = model_pytorch(model,criterion,optimizer,train_loader,val_loader,device,epochs = 500)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 1\n",
            "training loss: 617.7017, acc 0.0000 \n",
            "validation loss: 594.3564, validation acc 0.0000 \n",
            "epoch : 2\n",
            "training loss: 539.6104, acc 0.0000 \n",
            "validation loss: 473.0209, validation acc 0.0000 \n",
            "epoch : 3\n",
            "training loss: 378.6033, acc 0.0002 \n",
            "validation loss: 276.0556, validation acc 0.0016 \n",
            "epoch : 4\n",
            "training loss: 181.3769, acc 0.0133 \n",
            "validation loss: 123.4724, validation acc 0.0144 \n",
            "epoch : 5\n",
            "training loss: 120.3614, acc 0.0140 \n",
            "validation loss: 120.3182, validation acc 0.0145 \n",
            "epoch : 6\n",
            "training loss: 120.1435, acc 0.0136 \n",
            "validation loss: 120.2133, validation acc 0.0134 \n",
            "epoch : 7\n",
            "training loss: 120.1050, acc 0.0142 \n",
            "validation loss: 120.3365, validation acc 0.0128 \n",
            "epoch : 8\n",
            "training loss: 120.1137, acc 0.0138 \n",
            "validation loss: 120.2321, validation acc 0.0131 \n",
            "epoch : 9\n",
            "training loss: 120.1195, acc 0.0135 \n",
            "validation loss: 120.2612, validation acc 0.0137 \n",
            "epoch : 10\n",
            "training loss: 120.1103, acc 0.0141 \n",
            "validation loss: 120.2639, validation acc 0.0132 \n",
            "epoch : 11\n",
            "training loss: 120.1185, acc 0.0141 \n",
            "validation loss: 120.2565, validation acc 0.0144 \n",
            "epoch : 12\n",
            "training loss: 120.1187, acc 0.0141 \n",
            "validation loss: 120.1238, validation acc 0.0140 \n",
            "epoch : 13\n",
            "training loss: 120.0942, acc 0.0143 \n",
            "validation loss: 120.3047, validation acc 0.0138 \n",
            "epoch : 14\n",
            "training loss: 120.1495, acc 0.0139 \n",
            "validation loss: 120.3123, validation acc 0.0133 \n",
            "epoch : 15\n",
            "training loss: 120.1118, acc 0.0132 \n",
            "validation loss: 120.3055, validation acc 0.0141 \n",
            "epoch : 16\n",
            "training loss: 120.1055, acc 0.0136 \n",
            "validation loss: 120.3246, validation acc 0.0144 \n",
            "epoch : 17\n",
            "training loss: 120.0999, acc 0.0139 \n",
            "validation loss: 120.3237, validation acc 0.0144 \n",
            "epoch : 18\n",
            "training loss: 120.1141, acc 0.0140 \n",
            "validation loss: 120.3116, validation acc 0.0132 \n",
            "epoch : 19\n",
            "training loss: 120.1456, acc 0.0134 \n",
            "validation loss: 120.2553, validation acc 0.0132 \n",
            "epoch : 20\n",
            "training loss: 120.1144, acc 0.0141 \n",
            "validation loss: 120.2760, validation acc 0.0140 \n",
            "epoch : 21\n",
            "training loss: 120.1123, acc 0.0142 \n",
            "validation loss: 120.2635, validation acc 0.0134 \n",
            "epoch : 22\n",
            "training loss: 120.1200, acc 0.0140 \n",
            "validation loss: 120.2870, validation acc 0.0131 \n",
            "epoch : 23\n",
            "training loss: 120.1047, acc 0.0140 \n",
            "validation loss: 120.3007, validation acc 0.0145 \n",
            "epoch : 24\n",
            "training loss: 120.1139, acc 0.0140 \n",
            "validation loss: 120.2764, validation acc 0.0139 \n",
            "epoch : 25\n",
            "training loss: 120.1059, acc 0.0138 \n",
            "validation loss: 120.2205, validation acc 0.0138 \n",
            "epoch : 26\n",
            "training loss: 120.1031, acc 0.0139 \n",
            "validation loss: 120.3382, validation acc 0.0135 \n",
            "epoch : 27\n",
            "training loss: 120.1473, acc 0.0139 \n",
            "validation loss: 120.2268, validation acc 0.0148 \n",
            "epoch : 28\n",
            "training loss: 120.1307, acc 0.0138 \n",
            "validation loss: 120.2022, validation acc 0.0136 \n",
            "epoch : 29\n",
            "training loss: 120.1514, acc 0.0139 \n",
            "validation loss: 120.2544, validation acc 0.0154 \n",
            "epoch : 30\n",
            "training loss: 120.1045, acc 0.0135 \n",
            "validation loss: 120.2106, validation acc 0.0139 \n",
            "epoch : 31\n",
            "training loss: 120.1498, acc 0.0139 \n",
            "validation loss: 120.2644, validation acc 0.0137 \n",
            "epoch : 32\n",
            "training loss: 120.1144, acc 0.0143 \n",
            "validation loss: 120.2805, validation acc 0.0134 \n",
            "epoch : 33\n",
            "training loss: 120.1017, acc 0.0133 \n",
            "validation loss: 120.2993, validation acc 0.0136 \n",
            "epoch : 34\n",
            "training loss: 120.1004, acc 0.0141 \n",
            "validation loss: 120.2726, validation acc 0.0139 \n",
            "epoch : 35\n",
            "training loss: 120.1201, acc 0.0134 \n",
            "validation loss: 120.1890, validation acc 0.0138 \n",
            "epoch : 36\n",
            "training loss: 120.1013, acc 0.0135 \n",
            "validation loss: 120.2032, validation acc 0.0131 \n",
            "epoch : 37\n",
            "training loss: 120.1103, acc 0.0136 \n",
            "validation loss: 120.1843, validation acc 0.0151 \n",
            "epoch : 38\n",
            "training loss: 120.1107, acc 0.0141 \n",
            "validation loss: 120.2666, validation acc 0.0137 \n",
            "epoch : 39\n",
            "training loss: 120.1217, acc 0.0145 \n",
            "validation loss: 120.3109, validation acc 0.0143 \n",
            "epoch : 40\n",
            "training loss: 120.1071, acc 0.0141 \n",
            "validation loss: 120.2813, validation acc 0.0124 \n",
            "epoch : 41\n",
            "training loss: 120.1134, acc 0.0137 \n",
            "validation loss: 120.2123, validation acc 0.0136 \n",
            "epoch : 42\n",
            "training loss: 120.1365, acc 0.0138 \n",
            "validation loss: 120.2789, validation acc 0.0147 \n",
            "epoch : 43\n",
            "training loss: 120.1251, acc 0.0133 \n",
            "validation loss: 120.3027, validation acc 0.0136 \n",
            "epoch : 44\n",
            "training loss: 120.1037, acc 0.0140 \n",
            "validation loss: 120.2116, validation acc 0.0148 \n",
            "epoch : 45\n",
            "training loss: 120.1417, acc 0.0138 \n",
            "validation loss: 120.3031, validation acc 0.0128 \n",
            "epoch : 46\n",
            "training loss: 120.1016, acc 0.0136 \n",
            "validation loss: 120.2232, validation acc 0.0129 \n",
            "epoch : 47\n",
            "training loss: 120.1269, acc 0.0138 \n",
            "validation loss: 120.2332, validation acc 0.0144 \n",
            "epoch : 48\n",
            "training loss: 120.1340, acc 0.0136 \n",
            "validation loss: 120.2320, validation acc 0.0130 \n",
            "epoch : 49\n",
            "training loss: 120.0871, acc 0.0138 \n",
            "validation loss: 120.3019, validation acc 0.0136 \n",
            "epoch : 50\n",
            "training loss: 120.1451, acc 0.0136 \n",
            "validation loss: 120.3186, validation acc 0.0140 \n",
            "epoch : 51\n",
            "training loss: 120.1462, acc 0.0136 \n",
            "validation loss: 120.3091, validation acc 0.0155 \n",
            "epoch : 52\n",
            "training loss: 120.1894, acc 0.0141 \n",
            "validation loss: 120.1865, validation acc 0.0147 \n",
            "epoch : 53\n",
            "training loss: 120.1115, acc 0.0136 \n",
            "validation loss: 120.2961, validation acc 0.0141 \n",
            "epoch : 54\n",
            "training loss: 120.1304, acc 0.0139 \n",
            "validation loss: 120.2954, validation acc 0.0143 \n",
            "epoch : 55\n",
            "training loss: 120.1082, acc 0.0130 \n",
            "validation loss: 120.2911, validation acc 0.0136 \n",
            "epoch : 56\n",
            "training loss: 120.1100, acc 0.0142 \n",
            "validation loss: 120.2250, validation acc 0.0141 \n",
            "epoch : 57\n",
            "training loss: 120.1272, acc 0.0137 \n",
            "validation loss: 120.2386, validation acc 0.0138 \n",
            "epoch : 58\n",
            "training loss: 120.1070, acc 0.0135 \n",
            "validation loss: 120.2075, validation acc 0.0134 \n",
            "epoch : 59\n",
            "training loss: 120.0622, acc 0.0136 \n",
            "validation loss: 120.2772, validation acc 0.0134 \n",
            "epoch : 60\n",
            "training loss: 120.1346, acc 0.0135 \n",
            "validation loss: 120.3123, validation acc 0.0130 \n",
            "epoch : 61\n",
            "training loss: 120.1932, acc 0.0135 \n",
            "validation loss: 120.1433, validation acc 0.0141 \n",
            "epoch : 62\n",
            "training loss: 120.0908, acc 0.0146 \n",
            "validation loss: 120.2967, validation acc 0.0141 \n",
            "epoch : 63\n",
            "training loss: 120.1318, acc 0.0133 \n",
            "validation loss: 120.2949, validation acc 0.0157 \n",
            "epoch : 64\n",
            "training loss: 120.0906, acc 0.0136 \n",
            "validation loss: 120.2686, validation acc 0.0129 \n",
            "epoch : 65\n",
            "training loss: 120.1301, acc 0.0138 \n",
            "validation loss: 120.3491, validation acc 0.0134 \n",
            "epoch : 66\n",
            "training loss: 120.1108, acc 0.0139 \n",
            "validation loss: 120.2564, validation acc 0.0143 \n",
            "epoch : 67\n",
            "training loss: 120.1486, acc 0.0134 \n",
            "validation loss: 120.2516, validation acc 0.0146 \n",
            "epoch : 68\n",
            "training loss: 120.0792, acc 0.0136 \n",
            "validation loss: 120.2922, validation acc 0.0147 \n",
            "epoch : 69\n",
            "training loss: 120.0830, acc 0.0137 \n",
            "validation loss: 120.2224, validation acc 0.0132 \n",
            "epoch : 70\n",
            "training loss: 120.1459, acc 0.0143 \n",
            "validation loss: 120.2437, validation acc 0.0144 \n",
            "epoch : 71\n",
            "training loss: 120.1125, acc 0.0140 \n",
            "validation loss: 120.2332, validation acc 0.0140 \n",
            "epoch : 72\n",
            "training loss: 120.1020, acc 0.0137 \n",
            "validation loss: 120.2574, validation acc 0.0145 \n",
            "epoch : 73\n",
            "training loss: 120.0884, acc 0.0142 \n",
            "validation loss: 120.3197, validation acc 0.0157 \n",
            "epoch : 74\n",
            "training loss: 120.1174, acc 0.0136 \n",
            "validation loss: 120.2166, validation acc 0.0146 \n",
            "epoch : 75\n",
            "training loss: 120.1026, acc 0.0133 \n",
            "validation loss: 120.2087, validation acc 0.0142 \n",
            "epoch : 76\n",
            "training loss: 120.1065, acc 0.0135 \n",
            "validation loss: 120.2048, validation acc 0.0143 \n",
            "epoch : 77\n",
            "training loss: 120.0974, acc 0.0139 \n",
            "validation loss: 120.2224, validation acc 0.0133 \n",
            "epoch : 78\n",
            "training loss: 120.1250, acc 0.0137 \n",
            "validation loss: 120.2647, validation acc 0.0144 \n",
            "epoch : 79\n",
            "training loss: 120.1032, acc 0.0138 \n",
            "validation loss: 120.2664, validation acc 0.0134 \n",
            "epoch : 80\n",
            "training loss: 120.0717, acc 0.0143 \n",
            "validation loss: 120.2275, validation acc 0.0144 \n",
            "epoch : 81\n",
            "training loss: 120.0913, acc 0.0140 \n",
            "validation loss: 120.1723, validation acc 0.0144 \n",
            "epoch : 82\n",
            "training loss: 120.0451, acc 0.0141 \n",
            "validation loss: 120.2394, validation acc 0.0154 \n",
            "epoch : 83\n",
            "training loss: 120.1018, acc 0.0136 \n",
            "validation loss: 120.2430, validation acc 0.0156 \n",
            "epoch : 84\n",
            "training loss: 120.0838, acc 0.0138 \n",
            "validation loss: 120.2899, validation acc 0.0135 \n",
            "epoch : 85\n",
            "training loss: 120.0906, acc 0.0138 \n",
            "validation loss: 120.2136, validation acc 0.0136 \n",
            "epoch : 86\n",
            "training loss: 120.1377, acc 0.0135 \n",
            "validation loss: 120.1719, validation acc 0.0148 \n",
            "epoch : 87\n",
            "training loss: 120.0924, acc 0.0141 \n",
            "validation loss: 120.2823, validation acc 0.0136 \n",
            "epoch : 88\n",
            "training loss: 120.1028, acc 0.0137 \n",
            "validation loss: 120.2627, validation acc 0.0138 \n",
            "epoch : 89\n",
            "training loss: 120.1214, acc 0.0138 \n",
            "validation loss: 120.2550, validation acc 0.0138 \n",
            "epoch : 90\n",
            "training loss: 120.1089, acc 0.0140 \n",
            "validation loss: 120.2357, validation acc 0.0131 \n",
            "epoch : 91\n",
            "training loss: 120.0540, acc 0.0133 \n",
            "validation loss: 120.3119, validation acc 0.0129 \n",
            "epoch : 92\n",
            "training loss: 120.0719, acc 0.0141 \n",
            "validation loss: 120.2787, validation acc 0.0141 \n",
            "epoch : 93\n",
            "training loss: 120.1158, acc 0.0138 \n",
            "validation loss: 120.2580, validation acc 0.0150 \n",
            "epoch : 94\n",
            "training loss: 120.1009, acc 0.0142 \n",
            "validation loss: 120.2263, validation acc 0.0140 \n",
            "epoch : 95\n",
            "training loss: 120.0535, acc 0.0136 \n",
            "validation loss: 120.3170, validation acc 0.0124 \n",
            "epoch : 96\n",
            "training loss: 120.0778, acc 0.0135 \n",
            "validation loss: 120.2051, validation acc 0.0151 \n",
            "epoch : 97\n",
            "training loss: 120.0700, acc 0.0147 \n",
            "validation loss: 120.2057, validation acc 0.0146 \n",
            "epoch : 98\n",
            "training loss: 120.1007, acc 0.0137 \n",
            "validation loss: 120.1957, validation acc 0.0140 \n",
            "epoch : 99\n",
            "training loss: 120.1231, acc 0.0135 \n",
            "validation loss: 120.3305, validation acc 0.0133 \n",
            "epoch : 100\n",
            "training loss: 120.0465, acc 0.0147 \n",
            "validation loss: 120.1988, validation acc 0.0141 \n",
            "epoch : 101\n",
            "training loss: 120.0840, acc 0.0138 \n",
            "validation loss: 120.1611, validation acc 0.0153 \n",
            "epoch : 102\n",
            "training loss: 120.0672, acc 0.0143 \n",
            "validation loss: 120.2694, validation acc 0.0134 \n",
            "epoch : 103\n",
            "training loss: 120.0865, acc 0.0138 \n",
            "validation loss: 120.1382, validation acc 0.0140 \n",
            "epoch : 104\n",
            "training loss: 120.0668, acc 0.0141 \n",
            "validation loss: 120.2472, validation acc 0.0149 \n",
            "epoch : 105\n",
            "training loss: 120.0789, acc 0.0141 \n",
            "validation loss: 120.3517, validation acc 0.0128 \n",
            "epoch : 106\n",
            "training loss: 120.0726, acc 0.0139 \n",
            "validation loss: 120.1824, validation acc 0.0149 \n",
            "epoch : 107\n",
            "training loss: 120.1052, acc 0.0134 \n",
            "validation loss: 120.2644, validation acc 0.0147 \n",
            "epoch : 108\n",
            "training loss: 120.1057, acc 0.0138 \n",
            "validation loss: 120.1775, validation acc 0.0144 \n",
            "epoch : 109\n",
            "training loss: 120.0307, acc 0.0135 \n",
            "validation loss: 120.3025, validation acc 0.0133 \n",
            "epoch : 110\n",
            "training loss: 120.1009, acc 0.0143 \n",
            "validation loss: 120.2195, validation acc 0.0156 \n",
            "epoch : 111\n",
            "training loss: 120.0732, acc 0.0138 \n",
            "validation loss: 120.1943, validation acc 0.0144 \n",
            "epoch : 112\n",
            "training loss: 120.0670, acc 0.0133 \n",
            "validation loss: 120.1732, validation acc 0.0134 \n",
            "epoch : 113\n",
            "training loss: 120.0865, acc 0.0144 \n",
            "validation loss: 120.2100, validation acc 0.0139 \n",
            "epoch : 114\n",
            "training loss: 120.0771, acc 0.0135 \n",
            "validation loss: 120.1563, validation acc 0.0140 \n",
            "epoch : 115\n",
            "training loss: 120.0541, acc 0.0143 \n",
            "validation loss: 120.2011, validation acc 0.0148 \n",
            "epoch : 116\n",
            "training loss: 120.0828, acc 0.0138 \n",
            "validation loss: 120.2177, validation acc 0.0123 \n",
            "epoch : 117\n",
            "training loss: 120.0492, acc 0.0141 \n",
            "validation loss: 120.2527, validation acc 0.0132 \n",
            "epoch : 118\n",
            "training loss: 120.1018, acc 0.0137 \n",
            "validation loss: 120.2410, validation acc 0.0149 \n",
            "epoch : 119\n",
            "training loss: 120.0990, acc 0.0145 \n",
            "validation loss: 120.1817, validation acc 0.0137 \n",
            "epoch : 120\n",
            "training loss: 120.0548, acc 0.0140 \n",
            "validation loss: 120.1910, validation acc 0.0142 \n",
            "epoch : 121\n",
            "training loss: 120.0980, acc 0.0136 \n",
            "validation loss: 120.1593, validation acc 0.0135 \n",
            "epoch : 122\n",
            "training loss: 120.0409, acc 0.0138 \n",
            "validation loss: 120.1282, validation acc 0.0131 \n",
            "epoch : 123\n",
            "training loss: 120.0814, acc 0.0137 \n",
            "validation loss: 120.2613, validation acc 0.0133 \n",
            "epoch : 124\n",
            "training loss: 120.0613, acc 0.0143 \n",
            "validation loss: 120.1253, validation acc 0.0140 \n",
            "epoch : 125\n",
            "training loss: 120.0957, acc 0.0133 \n",
            "validation loss: 120.3953, validation acc 0.0134 \n",
            "epoch : 126\n",
            "training loss: 120.0910, acc 0.0139 \n",
            "validation loss: 120.1915, validation acc 0.0139 \n",
            "epoch : 127\n",
            "training loss: 120.0466, acc 0.0136 \n",
            "validation loss: 120.1613, validation acc 0.0131 \n",
            "epoch : 128\n",
            "training loss: 120.0619, acc 0.0138 \n",
            "validation loss: 120.3031, validation acc 0.0148 \n",
            "epoch : 129\n",
            "training loss: 120.0991, acc 0.0137 \n",
            "validation loss: 120.1611, validation acc 0.0143 \n",
            "epoch : 130\n",
            "training loss: 120.0659, acc 0.0129 \n",
            "validation loss: 120.1202, validation acc 0.0143 \n",
            "epoch : 131\n",
            "training loss: 120.0577, acc 0.0134 \n",
            "validation loss: 120.1999, validation acc 0.0144 \n",
            "epoch : 132\n",
            "training loss: 120.1069, acc 0.0136 \n",
            "validation loss: 120.1468, validation acc 0.0143 \n",
            "epoch : 133\n",
            "training loss: 120.0770, acc 0.0142 \n",
            "validation loss: 120.1761, validation acc 0.0144 \n",
            "epoch : 134\n",
            "training loss: 120.0290, acc 0.0140 \n",
            "validation loss: 120.1916, validation acc 0.0149 \n",
            "epoch : 135\n",
            "training loss: 120.0533, acc 0.0143 \n",
            "validation loss: 120.2434, validation acc 0.0133 \n",
            "epoch : 136\n",
            "training loss: 120.0710, acc 0.0137 \n",
            "validation loss: 120.1291, validation acc 0.0148 \n",
            "epoch : 137\n",
            "training loss: 120.0596, acc 0.0143 \n",
            "validation loss: 120.1631, validation acc 0.0144 \n",
            "epoch : 138\n",
            "training loss: 120.0491, acc 0.0141 \n",
            "validation loss: 120.1770, validation acc 0.0149 \n",
            "epoch : 139\n",
            "training loss: 120.0042, acc 0.0143 \n",
            "validation loss: 120.2281, validation acc 0.0159 \n",
            "epoch : 140\n",
            "training loss: 120.0475, acc 0.0137 \n",
            "validation loss: 120.2735, validation acc 0.0126 \n",
            "epoch : 141\n",
            "training loss: 120.0743, acc 0.0139 \n",
            "validation loss: 120.3388, validation acc 0.0132 \n",
            "epoch : 142\n",
            "training loss: 120.0523, acc 0.0137 \n",
            "validation loss: 120.1975, validation acc 0.0142 \n",
            "epoch : 143\n",
            "training loss: 120.0501, acc 0.0137 \n",
            "validation loss: 120.1152, validation acc 0.0140 \n",
            "epoch : 144\n",
            "training loss: 120.0153, acc 0.0144 \n",
            "validation loss: 120.2568, validation acc 0.0129 \n",
            "epoch : 145\n",
            "training loss: 120.0491, acc 0.0138 \n",
            "validation loss: 120.2213, validation acc 0.0144 \n",
            "epoch : 146\n",
            "training loss: 120.0630, acc 0.0140 \n",
            "validation loss: 120.1819, validation acc 0.0143 \n",
            "epoch : 147\n",
            "training loss: 120.0180, acc 0.0137 \n",
            "validation loss: 120.2985, validation acc 0.0136 \n",
            "epoch : 148\n",
            "training loss: 120.0693, acc 0.0132 \n",
            "validation loss: 120.1698, validation acc 0.0132 \n",
            "epoch : 149\n",
            "training loss: 120.0460, acc 0.0135 \n",
            "validation loss: 120.1793, validation acc 0.0140 \n",
            "epoch : 150\n",
            "training loss: 120.0506, acc 0.0140 \n",
            "validation loss: 120.2101, validation acc 0.0120 \n",
            "epoch : 151\n",
            "training loss: 120.0404, acc 0.0134 \n",
            "validation loss: 120.3613, validation acc 0.0134 \n",
            "epoch : 152\n",
            "training loss: 120.0213, acc 0.0140 \n",
            "validation loss: 120.1263, validation acc 0.0140 \n",
            "epoch : 153\n",
            "training loss: 120.0442, acc 0.0132 \n",
            "validation loss: 120.2211, validation acc 0.0132 \n",
            "epoch : 154\n",
            "training loss: 120.0401, acc 0.0138 \n",
            "validation loss: 120.2341, validation acc 0.0141 \n",
            "epoch : 155\n",
            "training loss: 120.0495, acc 0.0141 \n",
            "validation loss: 120.2343, validation acc 0.0126 \n",
            "epoch : 156\n",
            "training loss: 120.0539, acc 0.0134 \n",
            "validation loss: 120.1542, validation acc 0.0139 \n",
            "epoch : 157\n",
            "training loss: 120.0438, acc 0.0139 \n",
            "validation loss: 120.1989, validation acc 0.0151 \n",
            "epoch : 158\n",
            "training loss: 120.0349, acc 0.0133 \n",
            "validation loss: 120.1935, validation acc 0.0129 \n",
            "epoch : 159\n",
            "training loss: 120.0130, acc 0.0141 \n",
            "validation loss: 120.1755, validation acc 0.0151 \n",
            "epoch : 160\n",
            "training loss: 120.0523, acc 0.0141 \n",
            "validation loss: 120.2041, validation acc 0.0135 \n",
            "epoch : 161\n",
            "training loss: 120.0708, acc 0.0132 \n",
            "validation loss: 120.1480, validation acc 0.0137 \n",
            "epoch : 162\n",
            "training loss: 120.0483, acc 0.0139 \n",
            "validation loss: 120.1661, validation acc 0.0145 \n",
            "epoch : 163\n",
            "training loss: 120.0146, acc 0.0142 \n",
            "validation loss: 120.2022, validation acc 0.0135 \n",
            "epoch : 164\n",
            "training loss: 120.0422, acc 0.0143 \n",
            "validation loss: 120.1826, validation acc 0.0136 \n",
            "epoch : 165\n",
            "training loss: 120.0501, acc 0.0138 \n",
            "validation loss: 120.1203, validation acc 0.0141 \n",
            "epoch : 166\n",
            "training loss: 120.0247, acc 0.0130 \n",
            "validation loss: 120.1312, validation acc 0.0153 \n",
            "epoch : 167\n",
            "training loss: 120.0606, acc 0.0140 \n",
            "validation loss: 120.2039, validation acc 0.0134 \n",
            "epoch : 168\n",
            "training loss: 120.0642, acc 0.0138 \n",
            "validation loss: 120.1690, validation acc 0.0147 \n",
            "epoch : 169\n",
            "training loss: 119.9924, acc 0.0134 \n",
            "validation loss: 120.1938, validation acc 0.0138 \n",
            "epoch : 170\n",
            "training loss: 120.0676, acc 0.0139 \n",
            "validation loss: 120.2475, validation acc 0.0134 \n",
            "epoch : 171\n",
            "training loss: 120.0242, acc 0.0146 \n",
            "validation loss: 120.1920, validation acc 0.0146 \n",
            "epoch : 172\n",
            "training loss: 120.0109, acc 0.0139 \n",
            "validation loss: 120.2134, validation acc 0.0149 \n",
            "epoch : 173\n",
            "training loss: 120.0504, acc 0.0142 \n",
            "validation loss: 120.2778, validation acc 0.0134 \n",
            "epoch : 174\n",
            "training loss: 120.0395, acc 0.0139 \n",
            "validation loss: 120.2364, validation acc 0.0133 \n",
            "epoch : 175\n",
            "training loss: 120.0722, acc 0.0146 \n",
            "validation loss: 120.1923, validation acc 0.0135 \n",
            "epoch : 176\n",
            "training loss: 120.0596, acc 0.0139 \n",
            "validation loss: 120.2993, validation acc 0.0132 \n",
            "epoch : 177\n",
            "training loss: 120.0465, acc 0.0137 \n",
            "validation loss: 120.2422, validation acc 0.0137 \n",
            "epoch : 178\n",
            "training loss: 120.0285, acc 0.0134 \n",
            "validation loss: 120.1353, validation acc 0.0133 \n",
            "epoch : 179\n",
            "training loss: 120.0408, acc 0.0145 \n",
            "validation loss: 120.1370, validation acc 0.0143 \n",
            "epoch : 180\n",
            "training loss: 120.0412, acc 0.0138 \n",
            "validation loss: 120.1192, validation acc 0.0145 \n",
            "epoch : 181\n",
            "training loss: 120.0027, acc 0.0134 \n",
            "validation loss: 120.2107, validation acc 0.0135 \n",
            "epoch : 182\n",
            "training loss: 119.9846, acc 0.0135 \n",
            "validation loss: 120.2162, validation acc 0.0144 \n",
            "epoch : 183\n",
            "training loss: 120.0632, acc 0.0139 \n",
            "validation loss: 120.1737, validation acc 0.0144 \n",
            "epoch : 184\n",
            "training loss: 120.0332, acc 0.0139 \n",
            "validation loss: 120.1784, validation acc 0.0132 \n",
            "epoch : 185\n",
            "training loss: 120.0210, acc 0.0132 \n",
            "validation loss: 120.2178, validation acc 0.0144 \n",
            "epoch : 186\n",
            "training loss: 120.0242, acc 0.0139 \n",
            "validation loss: 120.1638, validation acc 0.0134 \n",
            "epoch : 187\n",
            "training loss: 120.0275, acc 0.0130 \n",
            "validation loss: 120.3207, validation acc 0.0135 \n",
            "epoch : 188\n",
            "training loss: 120.0408, acc 0.0137 \n",
            "validation loss: 120.1569, validation acc 0.0138 \n",
            "epoch : 189\n",
            "training loss: 120.0504, acc 0.0135 \n",
            "validation loss: 120.2131, validation acc 0.0131 \n",
            "epoch : 190\n",
            "training loss: 120.0911, acc 0.0145 \n",
            "validation loss: 120.1400, validation acc 0.0142 \n",
            "epoch : 191\n",
            "training loss: 119.9936, acc 0.0138 \n",
            "validation loss: 120.1819, validation acc 0.0148 \n",
            "epoch : 192\n",
            "training loss: 120.0538, acc 0.0135 \n",
            "validation loss: 120.1751, validation acc 0.0129 \n",
            "epoch : 193\n",
            "training loss: 120.0110, acc 0.0140 \n",
            "validation loss: 120.2413, validation acc 0.0134 \n",
            "epoch : 194\n",
            "training loss: 120.0195, acc 0.0138 \n",
            "validation loss: 120.1788, validation acc 0.0156 \n",
            "epoch : 195\n",
            "training loss: 120.0125, acc 0.0137 \n",
            "validation loss: 120.1655, validation acc 0.0137 \n",
            "epoch : 196\n",
            "training loss: 120.0404, acc 0.0141 \n",
            "validation loss: 120.1951, validation acc 0.0133 \n",
            "epoch : 197\n",
            "training loss: 120.0076, acc 0.0137 \n",
            "validation loss: 120.1827, validation acc 0.0136 \n",
            "epoch : 198\n",
            "training loss: 120.0167, acc 0.0136 \n",
            "validation loss: 120.1669, validation acc 0.0133 \n",
            "epoch : 199\n",
            "training loss: 120.0238, acc 0.0139 \n",
            "validation loss: 120.1308, validation acc 0.0148 \n",
            "epoch : 200\n",
            "training loss: 119.9924, acc 0.0137 \n",
            "validation loss: 120.1792, validation acc 0.0136 \n",
            "epoch : 201\n",
            "training loss: 120.0313, acc 0.0136 \n",
            "validation loss: 120.1981, validation acc 0.0131 \n",
            "epoch : 202\n",
            "training loss: 119.9792, acc 0.0140 \n",
            "validation loss: 120.1358, validation acc 0.0147 \n",
            "epoch : 203\n",
            "training loss: 120.0169, acc 0.0135 \n",
            "validation loss: 120.1762, validation acc 0.0123 \n",
            "epoch : 204\n",
            "training loss: 120.0075, acc 0.0141 \n",
            "validation loss: 120.0906, validation acc 0.0136 \n",
            "epoch : 205\n",
            "training loss: 120.0178, acc 0.0135 \n",
            "validation loss: 120.1638, validation acc 0.0149 \n",
            "epoch : 206\n",
            "training loss: 119.9737, acc 0.0140 \n",
            "validation loss: 120.2072, validation acc 0.0133 \n",
            "epoch : 207\n",
            "training loss: 120.0202, acc 0.0141 \n",
            "validation loss: 120.1635, validation acc 0.0151 \n",
            "epoch : 208\n",
            "training loss: 120.0307, acc 0.0131 \n",
            "validation loss: 120.1511, validation acc 0.0143 \n",
            "epoch : 209\n",
            "training loss: 120.0091, acc 0.0139 \n",
            "validation loss: 120.0861, validation acc 0.0141 \n",
            "epoch : 210\n",
            "training loss: 120.0111, acc 0.0139 \n",
            "validation loss: 120.1912, validation acc 0.0139 \n",
            "epoch : 211\n",
            "training loss: 119.9762, acc 0.0138 \n",
            "validation loss: 120.1454, validation acc 0.0134 \n",
            "epoch : 212\n",
            "training loss: 119.9896, acc 0.0139 \n",
            "validation loss: 120.2172, validation acc 0.0153 \n",
            "epoch : 213\n",
            "training loss: 120.0156, acc 0.0137 \n",
            "validation loss: 120.1476, validation acc 0.0130 \n",
            "epoch : 214\n",
            "training loss: 120.0197, acc 0.0135 \n",
            "validation loss: 120.1690, validation acc 0.0147 \n",
            "epoch : 215\n",
            "training loss: 119.9766, acc 0.0136 \n",
            "validation loss: 120.1454, validation acc 0.0149 \n",
            "epoch : 216\n",
            "training loss: 120.0085, acc 0.0143 \n",
            "validation loss: 120.1118, validation acc 0.0134 \n",
            "epoch : 217\n",
            "training loss: 120.0157, acc 0.0142 \n",
            "validation loss: 120.1142, validation acc 0.0149 \n",
            "epoch : 218\n",
            "training loss: 120.0377, acc 0.0132 \n",
            "validation loss: 120.1142, validation acc 0.0142 \n",
            "epoch : 219\n",
            "training loss: 119.9874, acc 0.0135 \n",
            "validation loss: 120.1699, validation acc 0.0135 \n",
            "epoch : 220\n",
            "training loss: 120.0031, acc 0.0136 \n",
            "validation loss: 120.1413, validation acc 0.0147 \n",
            "epoch : 221\n",
            "training loss: 120.0377, acc 0.0137 \n",
            "validation loss: 120.2034, validation acc 0.0129 \n",
            "epoch : 222\n",
            "training loss: 119.9979, acc 0.0138 \n",
            "validation loss: 120.1642, validation acc 0.0130 \n",
            "epoch : 223\n",
            "training loss: 120.0079, acc 0.0138 \n",
            "validation loss: 120.1295, validation acc 0.0140 \n",
            "epoch : 224\n",
            "training loss: 119.9994, acc 0.0139 \n",
            "validation loss: 120.2287, validation acc 0.0145 \n",
            "epoch : 225\n",
            "training loss: 120.0053, acc 0.0141 \n",
            "validation loss: 120.2230, validation acc 0.0128 \n",
            "epoch : 226\n",
            "training loss: 119.9867, acc 0.0137 \n",
            "validation loss: 120.0991, validation acc 0.0148 \n",
            "epoch : 227\n",
            "training loss: 120.0155, acc 0.0137 \n",
            "validation loss: 120.1162, validation acc 0.0133 \n",
            "epoch : 228\n",
            "training loss: 120.0095, acc 0.0137 \n",
            "validation loss: 120.1788, validation acc 0.0149 \n",
            "epoch : 229\n",
            "training loss: 119.9829, acc 0.0140 \n",
            "validation loss: 120.0850, validation acc 0.0139 \n",
            "epoch : 230\n",
            "training loss: 119.9935, acc 0.0136 \n",
            "validation loss: 120.1112, validation acc 0.0131 \n",
            "epoch : 231\n",
            "training loss: 119.9515, acc 0.0140 \n",
            "validation loss: 120.1767, validation acc 0.0151 \n",
            "epoch : 232\n",
            "training loss: 119.9963, acc 0.0139 \n",
            "validation loss: 120.1305, validation acc 0.0138 \n",
            "epoch : 233\n",
            "training loss: 119.9684, acc 0.0142 \n",
            "validation loss: 120.0938, validation acc 0.0142 \n",
            "epoch : 234\n",
            "training loss: 119.9780, acc 0.0137 \n",
            "validation loss: 120.2891, validation acc 0.0131 \n",
            "epoch : 235\n",
            "training loss: 120.0003, acc 0.0137 \n",
            "validation loss: 120.1277, validation acc 0.0141 \n",
            "epoch : 236\n",
            "training loss: 119.9616, acc 0.0139 \n",
            "validation loss: 120.1333, validation acc 0.0148 \n",
            "epoch : 237\n",
            "training loss: 120.0282, acc 0.0142 \n",
            "validation loss: 120.1175, validation acc 0.0128 \n",
            "epoch : 238\n",
            "training loss: 119.9559, acc 0.0142 \n",
            "validation loss: 120.1107, validation acc 0.0138 \n",
            "epoch : 239\n",
            "training loss: 120.0000, acc 0.0141 \n",
            "validation loss: 120.1385, validation acc 0.0144 \n",
            "epoch : 240\n",
            "training loss: 119.9658, acc 0.0134 \n",
            "validation loss: 120.1352, validation acc 0.0143 \n",
            "epoch : 241\n",
            "training loss: 119.9747, acc 0.0138 \n",
            "validation loss: 120.1589, validation acc 0.0135 \n",
            "epoch : 242\n",
            "training loss: 119.9484, acc 0.0138 \n",
            "validation loss: 120.1593, validation acc 0.0135 \n",
            "epoch : 243\n",
            "training loss: 119.9838, acc 0.0138 \n",
            "validation loss: 120.1173, validation acc 0.0131 \n",
            "epoch : 244\n",
            "training loss: 120.0058, acc 0.0128 \n",
            "validation loss: 120.0544, validation acc 0.0139 \n",
            "epoch : 245\n",
            "training loss: 120.0549, acc 0.0143 \n",
            "validation loss: 120.1273, validation acc 0.0136 \n",
            "epoch : 246\n",
            "training loss: 120.0160, acc 0.0138 \n",
            "validation loss: 120.1458, validation acc 0.0134 \n",
            "epoch : 247\n",
            "training loss: 119.9960, acc 0.0140 \n",
            "validation loss: 120.0797, validation acc 0.0149 \n",
            "epoch : 248\n",
            "training loss: 119.9987, acc 0.0134 \n",
            "validation loss: 120.1364, validation acc 0.0146 \n",
            "epoch : 249\n",
            "training loss: 120.0089, acc 0.0139 \n",
            "validation loss: 120.1955, validation acc 0.0133 \n",
            "epoch : 250\n",
            "training loss: 120.0041, acc 0.0139 \n",
            "validation loss: 120.1194, validation acc 0.0135 \n",
            "epoch : 251\n",
            "training loss: 119.9785, acc 0.0140 \n",
            "validation loss: 120.1687, validation acc 0.0138 \n",
            "epoch : 252\n",
            "training loss: 119.9571, acc 0.0140 \n",
            "validation loss: 120.2088, validation acc 0.0149 \n",
            "epoch : 253\n",
            "training loss: 119.9630, acc 0.0139 \n",
            "validation loss: 120.1674, validation acc 0.0131 \n",
            "epoch : 254\n",
            "training loss: 120.0071, acc 0.0138 \n",
            "validation loss: 120.1066, validation acc 0.0143 \n",
            "epoch : 255\n",
            "training loss: 120.0154, acc 0.0135 \n",
            "validation loss: 120.1215, validation acc 0.0144 \n",
            "epoch : 256\n",
            "training loss: 119.9908, acc 0.0134 \n",
            "validation loss: 120.0283, validation acc 0.0143 \n",
            "epoch : 257\n",
            "training loss: 119.9932, acc 0.0137 \n",
            "validation loss: 120.2103, validation acc 0.0126 \n",
            "epoch : 258\n",
            "training loss: 119.9623, acc 0.0134 \n",
            "validation loss: 120.1639, validation acc 0.0144 \n",
            "epoch : 259\n",
            "training loss: 120.0085, acc 0.0143 \n",
            "validation loss: 120.0835, validation acc 0.0156 \n",
            "epoch : 260\n",
            "training loss: 119.9708, acc 0.0137 \n",
            "validation loss: 120.0798, validation acc 0.0134 \n",
            "epoch : 261\n",
            "training loss: 119.9705, acc 0.0137 \n",
            "validation loss: 120.1310, validation acc 0.0142 \n",
            "epoch : 262\n",
            "training loss: 119.9420, acc 0.0140 \n",
            "validation loss: 120.0934, validation acc 0.0133 \n",
            "epoch : 263\n",
            "training loss: 119.9658, acc 0.0138 \n",
            "validation loss: 120.0692, validation acc 0.0151 \n",
            "epoch : 264\n",
            "training loss: 120.0015, acc 0.0138 \n",
            "validation loss: 120.1277, validation acc 0.0144 \n",
            "epoch : 265\n",
            "training loss: 119.9569, acc 0.0140 \n",
            "validation loss: 120.0912, validation acc 0.0141 \n",
            "epoch : 266\n",
            "training loss: 120.0001, acc 0.0141 \n",
            "validation loss: 120.2260, validation acc 0.0128 \n",
            "epoch : 267\n",
            "training loss: 119.9806, acc 0.0136 \n",
            "validation loss: 120.0948, validation acc 0.0135 \n",
            "epoch : 268\n",
            "training loss: 120.0095, acc 0.0137 \n",
            "validation loss: 120.2077, validation acc 0.0130 \n",
            "epoch : 269\n",
            "training loss: 119.9930, acc 0.0139 \n",
            "validation loss: 120.2543, validation acc 0.0133 \n",
            "epoch : 270\n",
            "training loss: 120.0088, acc 0.0143 \n",
            "validation loss: 120.0278, validation acc 0.0150 \n",
            "epoch : 271\n",
            "training loss: 119.9730, acc 0.0138 \n",
            "validation loss: 120.0731, validation acc 0.0138 \n",
            "epoch : 272\n",
            "training loss: 119.9629, acc 0.0141 \n",
            "validation loss: 120.1495, validation acc 0.0124 \n",
            "epoch : 273\n",
            "training loss: 119.9769, acc 0.0140 \n",
            "validation loss: 120.1470, validation acc 0.0133 \n",
            "epoch : 274\n",
            "training loss: 120.0002, acc 0.0135 \n",
            "validation loss: 120.1341, validation acc 0.0143 \n",
            "epoch : 275\n",
            "training loss: 120.0000, acc 0.0138 \n",
            "validation loss: 120.0809, validation acc 0.0139 \n",
            "epoch : 276\n",
            "training loss: 119.9781, acc 0.0136 \n",
            "validation loss: 120.0622, validation acc 0.0138 \n",
            "epoch : 277\n",
            "training loss: 119.9662, acc 0.0137 \n",
            "validation loss: 120.0646, validation acc 0.0142 \n",
            "epoch : 278\n",
            "training loss: 119.9580, acc 0.0141 \n",
            "validation loss: 120.1860, validation acc 0.0131 \n",
            "epoch : 279\n",
            "training loss: 119.9575, acc 0.0141 \n",
            "validation loss: 120.2216, validation acc 0.0128 \n",
            "epoch : 280\n",
            "training loss: 119.9634, acc 0.0139 \n",
            "validation loss: 120.0658, validation acc 0.0143 \n",
            "epoch : 281\n",
            "training loss: 119.9761, acc 0.0138 \n",
            "validation loss: 120.0429, validation acc 0.0128 \n",
            "epoch : 282\n",
            "training loss: 119.9727, acc 0.0140 \n",
            "validation loss: 120.0346, validation acc 0.0134 \n",
            "epoch : 283\n",
            "training loss: 119.9645, acc 0.0140 \n",
            "validation loss: 120.1133, validation acc 0.0131 \n",
            "epoch : 284\n",
            "training loss: 119.9120, acc 0.0137 \n",
            "validation loss: 120.0039, validation acc 0.0142 \n",
            "epoch : 285\n",
            "training loss: 119.9663, acc 0.0135 \n",
            "validation loss: 120.1801, validation acc 0.0127 \n",
            "epoch : 286\n",
            "training loss: 119.9481, acc 0.0137 \n",
            "validation loss: 120.0830, validation acc 0.0132 \n",
            "epoch : 287\n",
            "training loss: 119.9771, acc 0.0139 \n",
            "validation loss: 120.1239, validation acc 0.0133 \n",
            "epoch : 288\n",
            "training loss: 119.9602, acc 0.0141 \n",
            "validation loss: 120.0665, validation acc 0.0138 \n",
            "epoch : 289\n",
            "training loss: 119.9952, acc 0.0136 \n",
            "validation loss: 120.0011, validation acc 0.0132 \n",
            "epoch : 290\n",
            "training loss: 119.9680, acc 0.0138 \n",
            "validation loss: 120.1002, validation acc 0.0141 \n",
            "epoch : 291\n",
            "training loss: 119.9773, acc 0.0134 \n",
            "validation loss: 120.1581, validation acc 0.0145 \n",
            "epoch : 292\n",
            "training loss: 119.9707, acc 0.0140 \n",
            "validation loss: 120.0550, validation acc 0.0144 \n",
            "epoch : 293\n",
            "training loss: 119.9552, acc 0.0142 \n",
            "validation loss: 120.0833, validation acc 0.0146 \n",
            "epoch : 294\n",
            "training loss: 119.9657, acc 0.0142 \n",
            "validation loss: 120.2155, validation acc 0.0141 \n",
            "epoch : 295\n",
            "training loss: 119.9600, acc 0.0137 \n",
            "validation loss: 120.1222, validation acc 0.0131 \n",
            "epoch : 296\n",
            "training loss: 119.9870, acc 0.0136 \n",
            "validation loss: 120.0354, validation acc 0.0140 \n",
            "epoch : 297\n",
            "training loss: 119.9401, acc 0.0138 \n",
            "validation loss: 120.1160, validation acc 0.0138 \n",
            "epoch : 298\n",
            "training loss: 119.9538, acc 0.0146 \n",
            "validation loss: 120.0782, validation acc 0.0149 \n",
            "epoch : 299\n",
            "training loss: 119.9382, acc 0.0137 \n",
            "validation loss: 120.0892, validation acc 0.0142 \n",
            "epoch : 300\n",
            "training loss: 119.9795, acc 0.0138 \n",
            "validation loss: 120.1413, validation acc 0.0130 \n",
            "epoch : 301\n",
            "training loss: 119.9514, acc 0.0140 \n",
            "validation loss: 120.0746, validation acc 0.0139 \n",
            "epoch : 302\n",
            "training loss: 119.9253, acc 0.0137 \n",
            "validation loss: 120.0848, validation acc 0.0141 \n",
            "epoch : 303\n",
            "training loss: 119.9739, acc 0.0136 \n",
            "validation loss: 120.0784, validation acc 0.0154 \n",
            "epoch : 304\n",
            "training loss: 119.9534, acc 0.0138 \n",
            "validation loss: 120.1020, validation acc 0.0140 \n",
            "epoch : 305\n",
            "training loss: 119.9588, acc 0.0145 \n",
            "validation loss: 120.3004, validation acc 0.0135 \n",
            "epoch : 306\n",
            "training loss: 119.9399, acc 0.0142 \n",
            "validation loss: 120.2462, validation acc 0.0133 \n",
            "epoch : 307\n",
            "training loss: 119.9087, acc 0.0145 \n",
            "validation loss: 120.1871, validation acc 0.0132 \n",
            "epoch : 308\n",
            "training loss: 119.9514, acc 0.0134 \n",
            "validation loss: 120.0532, validation acc 0.0145 \n",
            "epoch : 309\n",
            "training loss: 119.9367, acc 0.0139 \n",
            "validation loss: 120.1393, validation acc 0.0155 \n",
            "epoch : 310\n",
            "training loss: 119.9675, acc 0.0138 \n",
            "validation loss: 120.1104, validation acc 0.0139 \n",
            "epoch : 311\n",
            "training loss: 119.9130, acc 0.0136 \n",
            "validation loss: 120.0899, validation acc 0.0130 \n",
            "epoch : 312\n",
            "training loss: 119.9659, acc 0.0139 \n",
            "validation loss: 120.1566, validation acc 0.0140 \n",
            "epoch : 313\n",
            "training loss: 119.9369, acc 0.0138 \n",
            "validation loss: 120.1537, validation acc 0.0123 \n",
            "epoch : 314\n",
            "training loss: 119.9478, acc 0.0138 \n",
            "validation loss: 120.0383, validation acc 0.0139 \n",
            "epoch : 315\n",
            "training loss: 119.9623, acc 0.0143 \n",
            "validation loss: 120.0757, validation acc 0.0140 \n",
            "epoch : 316\n",
            "training loss: 119.9359, acc 0.0141 \n",
            "validation loss: 120.1200, validation acc 0.0140 \n",
            "epoch : 317\n",
            "training loss: 119.9183, acc 0.0138 \n",
            "validation loss: 120.1260, validation acc 0.0146 \n",
            "epoch : 318\n",
            "training loss: 119.9186, acc 0.0141 \n",
            "validation loss: 120.0608, validation acc 0.0143 \n",
            "epoch : 319\n",
            "training loss: 119.9510, acc 0.0139 \n",
            "validation loss: 120.0654, validation acc 0.0135 \n",
            "epoch : 320\n",
            "training loss: 119.9098, acc 0.0140 \n",
            "validation loss: 120.0241, validation acc 0.0131 \n",
            "epoch : 321\n",
            "training loss: 119.9271, acc 0.0138 \n",
            "validation loss: 120.1081, validation acc 0.0146 \n",
            "epoch : 322\n",
            "training loss: 119.9342, acc 0.0131 \n",
            "validation loss: 120.1210, validation acc 0.0137 \n",
            "epoch : 323\n",
            "training loss: 119.9507, acc 0.0136 \n",
            "validation loss: 120.1608, validation acc 0.0128 \n",
            "epoch : 324\n",
            "training loss: 119.9638, acc 0.0135 \n",
            "validation loss: 120.0486, validation acc 0.0138 \n",
            "epoch : 325\n",
            "training loss: 119.9723, acc 0.0138 \n",
            "validation loss: 120.0670, validation acc 0.0144 \n",
            "epoch : 326\n",
            "training loss: 119.9454, acc 0.0139 \n",
            "validation loss: 120.0388, validation acc 0.0136 \n",
            "epoch : 327\n",
            "training loss: 119.9353, acc 0.0140 \n",
            "validation loss: 120.0079, validation acc 0.0141 \n",
            "epoch : 328\n",
            "training loss: 119.9353, acc 0.0138 \n",
            "validation loss: 120.1238, validation acc 0.0135 \n",
            "epoch : 329\n",
            "training loss: 119.9496, acc 0.0142 \n",
            "validation loss: 119.9864, validation acc 0.0144 \n",
            "epoch : 330\n",
            "training loss: 119.9299, acc 0.0141 \n",
            "validation loss: 120.0845, validation acc 0.0129 \n",
            "epoch : 331\n",
            "training loss: 119.9561, acc 0.0141 \n",
            "validation loss: 120.1647, validation acc 0.0152 \n",
            "epoch : 332\n",
            "training loss: 119.9136, acc 0.0136 \n",
            "validation loss: 120.0825, validation acc 0.0152 \n",
            "epoch : 333\n",
            "training loss: 119.9345, acc 0.0132 \n",
            "validation loss: 120.0870, validation acc 0.0133 \n",
            "epoch : 334\n",
            "training loss: 119.9145, acc 0.0141 \n",
            "validation loss: 120.0990, validation acc 0.0140 \n",
            "epoch : 335\n",
            "training loss: 119.8993, acc 0.0140 \n",
            "validation loss: 120.0230, validation acc 0.0151 \n",
            "epoch : 336\n",
            "training loss: 119.9410, acc 0.0137 \n",
            "validation loss: 120.0735, validation acc 0.0136 \n",
            "epoch : 337\n",
            "training loss: 119.9530, acc 0.0137 \n",
            "validation loss: 120.0877, validation acc 0.0131 \n",
            "epoch : 338\n",
            "training loss: 119.8954, acc 0.0140 \n",
            "validation loss: 119.9786, validation acc 0.0147 \n",
            "epoch : 339\n",
            "training loss: 119.9164, acc 0.0140 \n",
            "validation loss: 120.0787, validation acc 0.0136 \n",
            "epoch : 340\n",
            "training loss: 119.9100, acc 0.0138 \n",
            "validation loss: 120.0701, validation acc 0.0130 \n",
            "epoch : 341\n",
            "training loss: 119.9309, acc 0.0146 \n",
            "validation loss: 120.1845, validation acc 0.0125 \n",
            "epoch : 342\n",
            "training loss: 119.9364, acc 0.0142 \n",
            "validation loss: 120.1324, validation acc 0.0138 \n",
            "epoch : 343\n",
            "training loss: 119.9084, acc 0.0145 \n",
            "validation loss: 120.2717, validation acc 0.0144 \n",
            "epoch : 344\n",
            "training loss: 119.9323, acc 0.0138 \n",
            "validation loss: 120.0567, validation acc 0.0143 \n",
            "epoch : 345\n",
            "training loss: 119.9098, acc 0.0140 \n",
            "validation loss: 120.1129, validation acc 0.0132 \n",
            "epoch : 346\n",
            "training loss: 119.9160, acc 0.0136 \n",
            "validation loss: 120.1212, validation acc 0.0131 \n",
            "epoch : 347\n",
            "training loss: 119.9730, acc 0.0135 \n",
            "validation loss: 120.1369, validation acc 0.0127 \n",
            "epoch : 348\n",
            "training loss: 119.9446, acc 0.0144 \n",
            "validation loss: 120.1548, validation acc 0.0129 \n",
            "epoch : 349\n",
            "training loss: 119.9182, acc 0.0139 \n",
            "validation loss: 120.0853, validation acc 0.0139 \n",
            "epoch : 350\n",
            "training loss: 119.9058, acc 0.0133 \n",
            "validation loss: 120.1124, validation acc 0.0143 \n",
            "epoch : 351\n",
            "training loss: 119.9063, acc 0.0138 \n",
            "validation loss: 119.9707, validation acc 0.0131 \n",
            "epoch : 352\n",
            "training loss: 119.9200, acc 0.0137 \n",
            "validation loss: 120.0454, validation acc 0.0141 \n",
            "epoch : 353\n",
            "training loss: 119.9187, acc 0.0137 \n",
            "validation loss: 120.0643, validation acc 0.0133 \n",
            "epoch : 354\n",
            "training loss: 119.9190, acc 0.0130 \n",
            "validation loss: 120.1047, validation acc 0.0138 \n",
            "epoch : 355\n",
            "training loss: 119.9346, acc 0.0142 \n",
            "validation loss: 119.9916, validation acc 0.0140 \n",
            "epoch : 356\n",
            "training loss: 119.8815, acc 0.0140 \n",
            "validation loss: 120.0854, validation acc 0.0144 \n",
            "epoch : 357\n",
            "training loss: 119.9090, acc 0.0139 \n",
            "validation loss: 120.1459, validation acc 0.0138 \n",
            "epoch : 358\n",
            "training loss: 119.9106, acc 0.0138 \n",
            "validation loss: 120.2074, validation acc 0.0131 \n",
            "epoch : 359\n",
            "training loss: 119.9688, acc 0.0140 \n",
            "validation loss: 120.1525, validation acc 0.0131 \n",
            "epoch : 360\n",
            "training loss: 119.9296, acc 0.0140 \n",
            "validation loss: 119.9592, validation acc 0.0141 \n",
            "epoch : 361\n",
            "training loss: 119.9148, acc 0.0137 \n",
            "validation loss: 120.0763, validation acc 0.0149 \n",
            "epoch : 362\n",
            "training loss: 119.8969, acc 0.0138 \n",
            "validation loss: 119.9806, validation acc 0.0152 \n",
            "epoch : 363\n",
            "training loss: 119.9171, acc 0.0135 \n",
            "validation loss: 120.1069, validation acc 0.0137 \n",
            "epoch : 364\n",
            "training loss: 119.9097, acc 0.0141 \n",
            "validation loss: 120.0399, validation acc 0.0139 \n",
            "epoch : 365\n",
            "training loss: 119.9541, acc 0.0140 \n",
            "validation loss: 120.0861, validation acc 0.0126 \n",
            "epoch : 366\n",
            "training loss: 119.9550, acc 0.0133 \n",
            "validation loss: 120.0789, validation acc 0.0134 \n",
            "epoch : 367\n",
            "training loss: 119.9270, acc 0.0139 \n",
            "validation loss: 120.0083, validation acc 0.0145 \n",
            "epoch : 368\n",
            "training loss: 119.9057, acc 0.0135 \n",
            "validation loss: 120.0537, validation acc 0.0138 \n",
            "epoch : 369\n",
            "training loss: 119.9015, acc 0.0141 \n",
            "validation loss: 119.9968, validation acc 0.0131 \n",
            "epoch : 370\n",
            "training loss: 119.9324, acc 0.0136 \n",
            "validation loss: 119.9680, validation acc 0.0156 \n",
            "epoch : 371\n",
            "training loss: 119.9148, acc 0.0134 \n",
            "validation loss: 120.0784, validation acc 0.0141 \n",
            "epoch : 372\n",
            "training loss: 119.8539, acc 0.0138 \n",
            "validation loss: 120.0251, validation acc 0.0134 \n",
            "epoch : 373\n",
            "training loss: 119.9171, acc 0.0129 \n",
            "validation loss: 120.0802, validation acc 0.0129 \n",
            "epoch : 374\n",
            "training loss: 119.9297, acc 0.0137 \n",
            "validation loss: 120.0306, validation acc 0.0140 \n",
            "epoch : 375\n",
            "training loss: 119.9127, acc 0.0134 \n",
            "validation loss: 120.0856, validation acc 0.0134 \n",
            "epoch : 376\n",
            "training loss: 119.9342, acc 0.0142 \n",
            "validation loss: 119.9936, validation acc 0.0137 \n",
            "epoch : 377\n",
            "training loss: 119.9249, acc 0.0138 \n",
            "validation loss: 120.0506, validation acc 0.0139 \n",
            "epoch : 378\n",
            "training loss: 119.9032, acc 0.0134 \n",
            "validation loss: 119.9929, validation acc 0.0141 \n",
            "epoch : 379\n",
            "training loss: 119.9252, acc 0.0136 \n",
            "validation loss: 120.0422, validation acc 0.0154 \n",
            "epoch : 380\n",
            "training loss: 119.8801, acc 0.0137 \n",
            "validation loss: 120.0073, validation acc 0.0136 \n",
            "epoch : 381\n",
            "training loss: 119.8961, acc 0.0134 \n",
            "validation loss: 120.1419, validation acc 0.0136 \n",
            "epoch : 382\n",
            "training loss: 119.9151, acc 0.0142 \n",
            "validation loss: 120.0440, validation acc 0.0139 \n",
            "epoch : 383\n",
            "training loss: 119.8905, acc 0.0134 \n",
            "validation loss: 119.9907, validation acc 0.0147 \n",
            "epoch : 384\n",
            "training loss: 119.9538, acc 0.0139 \n",
            "validation loss: 120.0843, validation acc 0.0136 \n",
            "epoch : 385\n",
            "training loss: 119.9080, acc 0.0141 \n",
            "validation loss: 120.0427, validation acc 0.0137 \n",
            "epoch : 386\n",
            "training loss: 119.9005, acc 0.0130 \n",
            "validation loss: 120.0805, validation acc 0.0144 \n",
            "epoch : 387\n",
            "training loss: 119.8535, acc 0.0140 \n",
            "validation loss: 120.0769, validation acc 0.0131 \n",
            "epoch : 388\n",
            "training loss: 119.9397, acc 0.0135 \n",
            "validation loss: 120.0338, validation acc 0.0133 \n",
            "epoch : 389\n",
            "training loss: 119.8955, acc 0.0138 \n",
            "validation loss: 120.0709, validation acc 0.0131 \n",
            "epoch : 390\n",
            "training loss: 119.9139, acc 0.0139 \n",
            "validation loss: 120.0821, validation acc 0.0134 \n",
            "epoch : 391\n",
            "training loss: 119.8936, acc 0.0140 \n",
            "validation loss: 120.0726, validation acc 0.0146 \n",
            "epoch : 392\n",
            "training loss: 119.8795, acc 0.0138 \n",
            "validation loss: 120.0430, validation acc 0.0153 \n",
            "epoch : 393\n",
            "training loss: 119.8672, acc 0.0139 \n",
            "validation loss: 120.0798, validation acc 0.0133 \n",
            "epoch : 394\n",
            "training loss: 119.8734, acc 0.0138 \n",
            "validation loss: 120.0011, validation acc 0.0144 \n",
            "epoch : 395\n",
            "training loss: 119.9080, acc 0.0140 \n",
            "validation loss: 120.0426, validation acc 0.0127 \n",
            "epoch : 396\n",
            "training loss: 119.8805, acc 0.0134 \n",
            "validation loss: 120.0864, validation acc 0.0133 \n",
            "epoch : 397\n",
            "training loss: 119.9009, acc 0.0137 \n",
            "validation loss: 120.1532, validation acc 0.0132 \n",
            "epoch : 398\n",
            "training loss: 119.9022, acc 0.0138 \n",
            "validation loss: 120.0255, validation acc 0.0136 \n",
            "epoch : 399\n",
            "training loss: 119.8636, acc 0.0136 \n",
            "validation loss: 119.9768, validation acc 0.0148 \n",
            "epoch : 400\n",
            "training loss: 119.8591, acc 0.0139 \n",
            "validation loss: 120.0033, validation acc 0.0149 \n",
            "epoch : 401\n",
            "training loss: 119.8905, acc 0.0139 \n",
            "validation loss: 120.0208, validation acc 0.0143 \n",
            "epoch : 402\n",
            "training loss: 119.8858, acc 0.0134 \n",
            "validation loss: 120.0111, validation acc 0.0128 \n",
            "epoch : 403\n",
            "training loss: 119.8920, acc 0.0139 \n",
            "validation loss: 119.9514, validation acc 0.0133 \n",
            "epoch : 404\n",
            "training loss: 119.8825, acc 0.0138 \n",
            "validation loss: 120.0099, validation acc 0.0148 \n",
            "epoch : 405\n",
            "training loss: 119.9136, acc 0.0145 \n",
            "validation loss: 120.0262, validation acc 0.0143 \n",
            "epoch : 406\n",
            "training loss: 119.8897, acc 0.0137 \n",
            "validation loss: 119.9945, validation acc 0.0135 \n",
            "epoch : 407\n",
            "training loss: 119.8857, acc 0.0137 \n",
            "validation loss: 119.9905, validation acc 0.0143 \n",
            "epoch : 408\n",
            "training loss: 119.8919, acc 0.0138 \n",
            "validation loss: 120.0496, validation acc 0.0137 \n",
            "epoch : 409\n",
            "training loss: 119.9101, acc 0.0137 \n",
            "validation loss: 119.9940, validation acc 0.0139 \n",
            "epoch : 410\n",
            "training loss: 119.8834, acc 0.0146 \n",
            "validation loss: 120.0152, validation acc 0.0140 \n",
            "epoch : 411\n",
            "training loss: 119.8840, acc 0.0142 \n",
            "validation loss: 120.0587, validation acc 0.0131 \n",
            "epoch : 412\n",
            "training loss: 119.8949, acc 0.0138 \n",
            "validation loss: 119.9783, validation acc 0.0145 \n",
            "epoch : 413\n",
            "training loss: 119.8853, acc 0.0133 \n",
            "validation loss: 120.0214, validation acc 0.0136 \n",
            "epoch : 414\n",
            "training loss: 119.8548, acc 0.0137 \n",
            "validation loss: 120.0682, validation acc 0.0142 \n",
            "epoch : 415\n",
            "training loss: 119.8891, acc 0.0129 \n",
            "validation loss: 120.0275, validation acc 0.0150 \n",
            "epoch : 416\n",
            "training loss: 119.8655, acc 0.0140 \n",
            "validation loss: 119.9810, validation acc 0.0149 \n",
            "epoch : 417\n",
            "training loss: 119.8832, acc 0.0135 \n",
            "validation loss: 119.9725, validation acc 0.0136 \n",
            "epoch : 418\n",
            "training loss: 119.8993, acc 0.0140 \n",
            "validation loss: 119.9774, validation acc 0.0132 \n",
            "epoch : 419\n",
            "training loss: 119.8623, acc 0.0143 \n",
            "validation loss: 120.0113, validation acc 0.0134 \n",
            "epoch : 420\n",
            "training loss: 119.8888, acc 0.0137 \n",
            "validation loss: 120.0706, validation acc 0.0125 \n",
            "epoch : 421\n",
            "training loss: 119.8929, acc 0.0138 \n",
            "validation loss: 119.9446, validation acc 0.0138 \n",
            "epoch : 422\n",
            "training loss: 119.8675, acc 0.0138 \n",
            "validation loss: 120.0911, validation acc 0.0130 \n",
            "epoch : 423\n",
            "training loss: 119.8577, acc 0.0141 \n",
            "validation loss: 120.0150, validation acc 0.0140 \n",
            "epoch : 424\n",
            "training loss: 119.8753, acc 0.0137 \n",
            "validation loss: 120.0891, validation acc 0.0129 \n",
            "epoch : 425\n",
            "training loss: 119.8852, acc 0.0139 \n",
            "validation loss: 120.0310, validation acc 0.0132 \n",
            "epoch : 426\n",
            "training loss: 119.8836, acc 0.0135 \n",
            "validation loss: 119.9933, validation acc 0.0137 \n",
            "epoch : 427\n",
            "training loss: 119.8823, acc 0.0131 \n",
            "validation loss: 119.9418, validation acc 0.0148 \n",
            "epoch : 428\n",
            "training loss: 119.8333, acc 0.0137 \n",
            "validation loss: 120.0643, validation acc 0.0135 \n",
            "epoch : 429\n",
            "training loss: 119.8621, acc 0.0140 \n",
            "validation loss: 120.0892, validation acc 0.0141 \n",
            "epoch : 430\n",
            "training loss: 119.8398, acc 0.0133 \n",
            "validation loss: 120.0087, validation acc 0.0125 \n",
            "epoch : 431\n",
            "training loss: 119.8460, acc 0.0143 \n",
            "validation loss: 120.0529, validation acc 0.0135 \n",
            "epoch : 432\n",
            "training loss: 119.8377, acc 0.0134 \n",
            "validation loss: 120.0080, validation acc 0.0138 \n",
            "epoch : 433\n",
            "training loss: 119.8493, acc 0.0138 \n",
            "validation loss: 120.0043, validation acc 0.0147 \n",
            "epoch : 434\n",
            "training loss: 119.8585, acc 0.0138 \n",
            "validation loss: 120.0742, validation acc 0.0140 \n",
            "epoch : 435\n",
            "training loss: 119.8516, acc 0.0134 \n",
            "validation loss: 120.0370, validation acc 0.0124 \n",
            "epoch : 436\n",
            "training loss: 119.8571, acc 0.0139 \n",
            "validation loss: 119.9165, validation acc 0.0129 \n",
            "epoch : 437\n",
            "training loss: 119.8674, acc 0.0136 \n",
            "validation loss: 120.0000, validation acc 0.0141 \n",
            "epoch : 438\n",
            "training loss: 119.8569, acc 0.0142 \n",
            "validation loss: 119.9437, validation acc 0.0138 \n",
            "epoch : 439\n",
            "training loss: 119.8797, acc 0.0141 \n",
            "validation loss: 119.9944, validation acc 0.0128 \n",
            "epoch : 440\n",
            "training loss: 119.8799, acc 0.0134 \n",
            "validation loss: 119.9756, validation acc 0.0144 \n",
            "epoch : 441\n",
            "training loss: 119.8402, acc 0.0135 \n",
            "validation loss: 120.0335, validation acc 0.0127 \n",
            "epoch : 442\n",
            "training loss: 119.8781, acc 0.0134 \n",
            "validation loss: 120.0961, validation acc 0.0131 \n",
            "epoch : 443\n",
            "training loss: 119.8815, acc 0.0137 \n",
            "validation loss: 120.1043, validation acc 0.0155 \n",
            "epoch : 444\n",
            "training loss: 119.8489, acc 0.0138 \n",
            "validation loss: 119.9914, validation acc 0.0133 \n",
            "epoch : 445\n",
            "training loss: 119.8412, acc 0.0135 \n",
            "validation loss: 120.0940, validation acc 0.0128 \n",
            "epoch : 446\n",
            "training loss: 119.8190, acc 0.0135 \n",
            "validation loss: 119.9107, validation acc 0.0144 \n",
            "epoch : 447\n",
            "training loss: 119.8843, acc 0.0141 \n",
            "validation loss: 120.0585, validation acc 0.0131 \n",
            "epoch : 448\n",
            "training loss: 119.8437, acc 0.0135 \n",
            "validation loss: 119.9269, validation acc 0.0127 \n",
            "epoch : 449\n",
            "training loss: 119.8449, acc 0.0141 \n",
            "validation loss: 120.0005, validation acc 0.0128 \n",
            "epoch : 450\n",
            "training loss: 119.8541, acc 0.0143 \n",
            "validation loss: 119.9730, validation acc 0.0139 \n",
            "epoch : 451\n",
            "training loss: 119.8677, acc 0.0138 \n",
            "validation loss: 119.8933, validation acc 0.0129 \n",
            "epoch : 452\n",
            "training loss: 119.8629, acc 0.0140 \n",
            "validation loss: 120.0090, validation acc 0.0140 \n",
            "epoch : 453\n",
            "training loss: 119.8862, acc 0.0136 \n",
            "validation loss: 120.0355, validation acc 0.0134 \n",
            "epoch : 454\n",
            "training loss: 119.8653, acc 0.0140 \n",
            "validation loss: 119.9883, validation acc 0.0136 \n",
            "epoch : 455\n",
            "training loss: 119.8675, acc 0.0136 \n",
            "validation loss: 119.9719, validation acc 0.0134 \n",
            "epoch : 456\n",
            "training loss: 119.8973, acc 0.0137 \n",
            "validation loss: 119.9812, validation acc 0.0134 \n",
            "epoch : 457\n",
            "training loss: 119.8336, acc 0.0137 \n",
            "validation loss: 120.1298, validation acc 0.0136 \n",
            "epoch : 458\n",
            "training loss: 119.8370, acc 0.0137 \n",
            "validation loss: 120.0101, validation acc 0.0127 \n",
            "epoch : 459\n",
            "training loss: 119.8728, acc 0.0137 \n",
            "validation loss: 119.9532, validation acc 0.0147 \n",
            "epoch : 460\n",
            "training loss: 119.8741, acc 0.0139 \n",
            "validation loss: 119.9193, validation acc 0.0136 \n",
            "epoch : 461\n",
            "training loss: 119.8133, acc 0.0143 \n",
            "validation loss: 119.9706, validation acc 0.0129 \n",
            "epoch : 462\n",
            "training loss: 119.9026, acc 0.0132 \n",
            "validation loss: 120.0703, validation acc 0.0133 \n",
            "epoch : 463\n",
            "training loss: 119.8648, acc 0.0137 \n",
            "validation loss: 120.0959, validation acc 0.0131 \n",
            "epoch : 464\n",
            "training loss: 119.8374, acc 0.0140 \n",
            "validation loss: 120.0574, validation acc 0.0134 \n",
            "epoch : 465\n",
            "training loss: 119.8613, acc 0.0131 \n",
            "validation loss: 120.0810, validation acc 0.0135 \n",
            "epoch : 466\n",
            "training loss: 119.8330, acc 0.0138 \n",
            "validation loss: 119.9903, validation acc 0.0144 \n",
            "epoch : 467\n",
            "training loss: 119.8579, acc 0.0143 \n",
            "validation loss: 119.9804, validation acc 0.0129 \n",
            "epoch : 468\n",
            "training loss: 119.8272, acc 0.0136 \n",
            "validation loss: 119.8713, validation acc 0.0143 \n",
            "epoch : 469\n",
            "training loss: 119.8892, acc 0.0136 \n",
            "validation loss: 120.0316, validation acc 0.0132 \n",
            "epoch : 470\n",
            "training loss: 119.8668, acc 0.0139 \n",
            "validation loss: 120.0208, validation acc 0.0133 \n",
            "epoch : 471\n",
            "training loss: 119.8314, acc 0.0135 \n",
            "validation loss: 119.9786, validation acc 0.0141 \n",
            "epoch : 472\n",
            "training loss: 119.8972, acc 0.0139 \n",
            "validation loss: 119.9937, validation acc 0.0154 \n",
            "epoch : 473\n",
            "training loss: 119.8755, acc 0.0144 \n",
            "validation loss: 119.9444, validation acc 0.0150 \n",
            "epoch : 474\n",
            "training loss: 119.8295, acc 0.0136 \n",
            "validation loss: 119.9046, validation acc 0.0148 \n",
            "epoch : 475\n",
            "training loss: 119.8510, acc 0.0136 \n",
            "validation loss: 119.9981, validation acc 0.0134 \n",
            "epoch : 476\n",
            "training loss: 119.8448, acc 0.0143 \n",
            "validation loss: 119.8873, validation acc 0.0144 \n",
            "epoch : 477\n",
            "training loss: 119.8457, acc 0.0138 \n",
            "validation loss: 120.0175, validation acc 0.0138 \n",
            "epoch : 478\n",
            "training loss: 119.8202, acc 0.0137 \n",
            "validation loss: 119.9724, validation acc 0.0131 \n",
            "epoch : 479\n",
            "training loss: 119.8319, acc 0.0139 \n",
            "validation loss: 120.0189, validation acc 0.0144 \n",
            "epoch : 480\n",
            "training loss: 119.8165, acc 0.0137 \n",
            "validation loss: 119.9652, validation acc 0.0124 \n",
            "epoch : 481\n",
            "training loss: 119.8079, acc 0.0139 \n",
            "validation loss: 119.9908, validation acc 0.0133 \n",
            "epoch : 482\n",
            "training loss: 119.8684, acc 0.0140 \n",
            "validation loss: 119.9528, validation acc 0.0148 \n",
            "epoch : 483\n",
            "training loss: 119.8409, acc 0.0138 \n",
            "validation loss: 119.9560, validation acc 0.0138 \n",
            "epoch : 484\n",
            "training loss: 119.8324, acc 0.0138 \n",
            "validation loss: 120.1021, validation acc 0.0124 \n",
            "epoch : 485\n",
            "training loss: 119.8733, acc 0.0143 \n",
            "validation loss: 120.0019, validation acc 0.0139 \n",
            "epoch : 486\n",
            "training loss: 119.8422, acc 0.0139 \n",
            "validation loss: 119.9600, validation acc 0.0135 \n",
            "epoch : 487\n",
            "training loss: 119.8366, acc 0.0137 \n",
            "validation loss: 119.9375, validation acc 0.0133 \n",
            "epoch : 488\n",
            "training loss: 119.8266, acc 0.0141 \n",
            "validation loss: 120.0843, validation acc 0.0133 \n",
            "epoch : 489\n",
            "training loss: 119.8623, acc 0.0136 \n",
            "validation loss: 119.9602, validation acc 0.0131 \n",
            "epoch : 490\n",
            "training loss: 119.8536, acc 0.0141 \n",
            "validation loss: 120.0014, validation acc 0.0135 \n",
            "epoch : 491\n",
            "training loss: 119.8312, acc 0.0134 \n",
            "validation loss: 119.9744, validation acc 0.0136 \n",
            "epoch : 492\n",
            "training loss: 119.8390, acc 0.0141 \n",
            "validation loss: 119.9472, validation acc 0.0138 \n",
            "epoch : 493\n",
            "training loss: 119.8137, acc 0.0143 \n",
            "validation loss: 119.9432, validation acc 0.0143 \n",
            "epoch : 494\n",
            "training loss: 119.8278, acc 0.0134 \n",
            "validation loss: 119.9000, validation acc 0.0154 \n",
            "epoch : 495\n",
            "training loss: 119.8620, acc 0.0135 \n",
            "validation loss: 119.9312, validation acc 0.0143 \n",
            "epoch : 496\n",
            "training loss: 119.8286, acc 0.0140 \n",
            "validation loss: 119.9635, validation acc 0.0130 \n",
            "epoch : 497\n",
            "training loss: 119.8486, acc 0.0139 \n",
            "validation loss: 119.9330, validation acc 0.0145 \n",
            "epoch : 498\n",
            "training loss: 119.8358, acc 0.0138 \n",
            "validation loss: 119.9833, validation acc 0.0143 \n",
            "epoch : 499\n",
            "training loss: 119.8061, acc 0.0136 \n",
            "validation loss: 119.9834, validation acc 0.0146 \n",
            "epoch : 500\n",
            "training loss: 119.8224, acc 0.0138 \n",
            "validation loss: 119.9432, validation acc 0.0143 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-mGUHVuiWyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7661d455-f3d0-4a21-95c6-a32e3d5bcb00"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 7475)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "galPGIlCvRqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_pytorch(model, test_loader):\n",
        "  sub = pd.DataFrame(columns=['id','y_pred_pytorch'])\n",
        "  with torch.no_grad():\n",
        "    #model.eval()\n",
        "    for inputs,idcol in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        #print(inputs.squeeze())\n",
        "        out = model(inputs.float()).squeeze().cpu().data.numpy()\n",
        "        idcol = idcol.cpu().data.numpy()\n",
        "        #print(out)\n",
        "        submission = pd.DataFrame({'id': idcol, 'y_pred_pytorch': out})\n",
        "        #print(submission)\n",
        "        sub = pd.concat([sub,submission])\n",
        "    return sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LkqhA0v3pJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub1 = predict_pytorch(model, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54jtTouEwUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "9a5066fc-9e29-4a45-e3ab-5cb209d30005"
      },
      "source": [
        "sub1.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>y_pred_pytorch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48035</td>\n",
              "      <td>623.910095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>87636</td>\n",
              "      <td>623.682861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50791</td>\n",
              "      <td>623.769897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42914</td>\n",
              "      <td>623.658081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35235</td>\n",
              "      <td>623.753784</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  y_pred_pytorch\n",
              "0  48035      623.910095\n",
              "1  87636      623.682861\n",
              "2  50791      623.769897\n",
              "3  42914      623.658081\n",
              "4  35235      623.753784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy9uhd1O6TWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "490ceb8c-0da5-471f-dc01-456617df12f9"
      },
      "source": [
        "sub = pd.merge(sub1,cleandf[['id','y']],on='id',how='left')\n",
        "sub['y_pred-y'] = (sub['y_pred_pytorch'] - sub['y']).abs()\n",
        "sub['result'] = sub['y_pred-y'].apply(lambda x: 0 if x>3 else 1)\n",
        "result_dict = dict(sub['result'].value_counts())\n",
        "result_dict[1]/(result_dict[0]+result_dict[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Y9o-stE0h2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "d5d73a2d-79ea-4e90-8d9d-22105b57e38a"
      },
      "source": [
        "sub.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>y_pred_pytorch</th>\n",
              "      <th>y</th>\n",
              "      <th>y_pred-y</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48035</td>\n",
              "      <td>623.910095</td>\n",
              "      <td>510</td>\n",
              "      <td>113.910095</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>87636</td>\n",
              "      <td>623.682861</td>\n",
              "      <td>721</td>\n",
              "      <td>97.317139</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50791</td>\n",
              "      <td>623.769897</td>\n",
              "      <td>398</td>\n",
              "      <td>225.769897</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42914</td>\n",
              "      <td>623.658081</td>\n",
              "      <td>529</td>\n",
              "      <td>94.658081</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35235</td>\n",
              "      <td>623.753784</td>\n",
              "      <td>815</td>\n",
              "      <td>191.246216</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  y_pred_pytorch    y    y_pred-y  result\n",
              "0  48035      623.910095  510  113.910095       0\n",
              "1  87636      623.682861  721   97.317139       0\n",
              "2  50791      623.769897  398  225.769897       0\n",
              "3  42914      623.658081  529   94.658081       0\n",
              "4  35235      623.753784  815  191.246216       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r-Q5xw97BrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d60db5a3-16b4-4277-9d40-765bea7e0016"
      },
      "source": [
        "result_dict"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 19720, 1: 280}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlKV_aTyEHfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}